
@article{kohliFittingLinearLinearPiecewise2015,
  title = {Fitting a {{Linear-Linear Piecewise Growth Mixture Model With Unknown Knots}}: {{A Comparison}} of {{Two Common Approaches}} to {{Inference}}},
  shorttitle = {Fitting a {{Linear-Linear Piecewise Growth Mixture Model With Unknown Knots}}},
  author = {Kohli, Nidhi and Hughes, John and Wang, Chun and Zopluoglu, Cengiz and Davison, Mark},
  year = {2015},
  month = apr,
  journal = {Psychological Methods},
  volume = {20},
  pages = {259--275},
  abstract = {A linear\textendash linear piecewise growth mixture model (PGMM) is appropriate for analyzing segmented (disjointed) change in individual behavior over time, where the data come from a mixture of 2 or more latent classes, and the underlying growth trajectories in the different segments of the developmental process within each latent class are linear. A PGMM allows the knot (change point), the time of transition from 1 phase (segment) to another, to be estimated (when it is not known a priori) along with the other model parameters. To assist researchers in deciding which estimation method is most advantageous for analyzing this kind of mixture data, the current research compares 2 popular approaches to inference for PGMMs: maximum likelihood (ML) via an expectation\textendash maximization (EM) algorithm, and Markov chain Monte Carlo (MCMC) for Bayesian inference. Monte Carlo simulations were carried out to investigate and compare the ability of the 2 approaches to recover the true parameters in linear\textendash linear PGMMs with unknown knots. The results show that MCMC for Bayesian inference outperformed ML via EM in nearly every simulation scenario. Real data examples are also presented, and the corresponding computer codes for model fitting are provided in the Appendix to aid practitioners who wish to apply this class of models.}
}


