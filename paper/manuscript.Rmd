---
title             : "Select relevant moderators in meta-regression using Bayesian penalization"
shorttitle        : "BAYESIAN REGULARIZED META-ANALYSIS"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1,2"
    corresponding: yes
    address: "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email: "c.j.vanlissa@uu.nl"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Programming front-end
  - name: "Sara van Erp"
    affiliation: "1"
    role:
      - Writing - Contributions and feedback
      - Programming back-end
affiliation:
  - id            : "1"
    institution   : "Utrecht University, dept. Methodology & Statistics"
  - id            : "2"
    institution   : "Open Science Community Utrecht"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!--
Skeleton lasso/pema paper
1.) What is Meta-analysis?
2.) What is meta-regression and how does it complement meta-analysis?
	a.) Introduce Moderators
	b.) Study Heterogeneity + Random Sampling Error (and their difference)	
2.5.) Fixed vs. random effects
	a.) Shortcomings of fixed effect models
3.) Shortcomings of current meta regressions w.r.t. estimating coefficients and heterogeneity:
	a.) Small sample size / overfitting
	b.) Non-normal data
4.) Various methods to estimate heterogeneity (and coefficients)
	a.) The use of WLS and REML
5.) Intro to Frequentist linear methods/Bayesian methods and Random forests, along with their 
(dis-)advantages:
	a.) Rma: uses WLS for estimation
	b.) MetaForest (Random Effects): Uses Random Forest Algorithm
	c.) Lasso Pema: uses penalized Lasso
	d.) Horseshoe Pema: uses horseshoe priors
6.) Goal of the current study
7.) Means of attaining goal and evaluation of performance:
	a.) simulation study
	b.) algorithmic performance
	c.) design factors
	d.) Impact of design factors on algorithim performance
	e.) Hypotheses of algorithmic performances

colour coding: I colour coded the text as to know from which file the text is copied
GREEN = derived from ‘Thesis_Metaforest’
BLUE = Thesis_lasso
BLACK = internship_report
RED = Inserted myself

# Introduction
--> 

Meta-analysis is a quantitative form of evidence synthesis,
whereby effect sizes from multiple similar studies are aggregated.
In its simplest form, this aggregation consists of the computation of a summary effect as a weighted average of the observed effect sizes.
This average is weighted to account for the fact that some observed effect sizes are assumed to be more informative about the underlying population effect.
Each effect size is assigned a weight that determines how influential it is in calculating the summary effect.
This weight is based on specific assumptions;
for example, the *fixed effect* model assumes that all observed effect sizes reflect one underlying true population effect size.
This assumption is well-suited to the situation where effect sizes from close replication studies are meta-analyzed [@higgins_re-evaluation_2009, fabrigar_conceptualizing_2016, maxwell_is_2015]. 
The *random effects* model, by contrast, assumes that population effect sizes follow a normal distribution.
Each observed effect size provides information about the mean and standard deviation of this distribution of population effect sizes.
This assumption is more appropriate when studies are conceptually similar and differences between them are random [@higgins_re-evaluation_2009, fabrigar_conceptualizing_2016, maxwell_is_2015]. 

Not all heterogeneity in effect sizes is random, however.
Quantifiable between-study differences may introduce systematic heterogeneity.
Such between-study differences are known as "moderators".
For example, if studies have been replicated in Europe and the Americas, this difference can be captured by a binary moderator called "continent".
Alternatively, if studies have used different dosages of the same drug, this may be captured by a continuous moderator called "dosage".
Systematic heterogeneity in the observed effect sizes can be accounted for using *meta-regression* (Viechtbauer \& López-López, 2015).
This technique provides estimates of the effect of one or more study characteristics on the overall effect size,
as well as of the overall effect size and residual heterogeneity after controlling for their influence.

One common application of meta-analysis is to summarize existing bodies of literature.
In such situations, the number of moderators is often relatively high because similar research questions have been studied in different laboratories,
using different methods, instruments, and samples.
Each of these between-study differences could be coded as a moderator, and some of these moderators may explain systematic heterogeneity.

It is theoretically possible to account for the influence of multiple moderators using meta-regression.
However, like any regression-based approach, meta-regression requires a relatively high number of cases (studies) per parameter obtain sufficient power to examine heterogeneity.
In practice the number of available studies is often too low to examine heterogeneity reliably (Riley, Higgins, & Deeks, 2011).
At the same time, there are many potential sources of heterogeneity,
as similar research questions are studied in different laboratories, using different methods,
instruments, and samples.
This leads to a problem known as the "curse of dimensionality": the number of candidate moderators is large relative to the number of cases in the data.
Such cases do not fit comfortably into the
classic meta-analysis paradigm, which, like any regression-based approach, requires a
high number of cases per parameter. 
Between-studies thus presents a non-trivial challenge to data aggregation
using classic meta-analytic methods.
At the same time, it also offers an unexploited
opportunity to learn which differences between studies have an impact on the effect size
found, if adequate exploratory techniques can be developed. 

Addressing the curse of dimensionality necessitates *variable selection*:
the selection of a smaller subset of relevant moderators from a larger number of candidate moderators.
One way to perform variable selection is by relying on theory.
However, in many fields of science, theories exist at the individual level of analysis (e.g., in social science, at the level of individual people).
These theories do not necessarily generalize to the study level of analysis.
Using theories at the individual level for moderator selection at the study level amounts to committing the ecological fallacy: generalizing inferences across levels of analysis [@jargowskyEcologicalFallacy2004].
To illustrate what theory at the study level of analysis might look like, 
consider the so-called *decline effect*.
It is a phenomenon whereby effect sizes in a particular tranche of the literature seem to diminish over time [@schoolerUnpublishedResultsHide2011].
It has been theorized that the decline effect can been attributed to regression to the mean:
A finding initially draws attention from the research community because an anomalously large effect size has been published, and subsequent replications find smaller effect sizes.
Based on the decline effect, we might thus expect the variable "year of publication" to be a relevant moderator of study effect sizes.
Note that this prediction is valid even if year is orthogonal to the outcome of interest within each study.
Until more theory about the drivers of between-study heterogeneity is developed,
however, this approach will have limited utility for variable selection.
<!--A second example that might illustrate this problem is a study that concluded that American college students' empathy had decreased over time, based on meta-analysis of 72 samples collected between 1979 and 2009.-->

An alternative solution is to rely on statistical methods for variable selection.
This is a focal issue in the discipline of machine learning [@hastieElementsStatisticalLearning2009].
One technique that facilitates variable selection is *regularization*:
shrinking model parameters towards zero, such that only larger parameters remain.
Although this technique biases the parameter estimates,
it also reduces their variance, which has the advantage of producing more generalizable results that make better predictions for new data [see @hastieElementsStatisticalLearning2009].
This paper introduces *Bayesian regularized meta-regression* (BRMA),
an algorithm that uses Bayesian estimation with regularizing priors to perform variable selection in meta-analysis.
The algorithm is implemented in the function `brma()` in the R-package `pema`.
<!--
Thus, there is a need of a regularization method to curtail overfitting. Least Absolute Shrinkage and Selection Operator [LASSO] (L1-norm regularization) can fulfill this role, since it has an advantage in terms of feature selection. The goal of this project is to implement L1-norm regularization in the weighted meta-regression, developing an new estimator for regularized meta-regression.
This dilemma could be resolved by an exploratory technique that can identify relevant moderators in meta-analysis,
and which is relatively robust to small sample sizes.
-->
<!--Such shrinkage is advantageous because it performs variable selection and results in a sparse model.
Sparse models are easier to interpret, as fewer non-zero effects remain.
Regularization also provides a solution to the curse of dimensionality; unlike standard regression models, it can provide identified models even if the number of moderators exceeds the number of cases.-->
<!--By doing this it automatically performs variable selection. It does not seem to be immediately clear why shrinking the coefficients should be an improvement to the model. However, by shrinking the parameters, it lowers the variance of the model by increasing the bias only a little bit. In other words, the model sacrifices some of its ability to fit the current data, to greatly increase the ability to predict future data with the same fit (James et al., 2013). This is better known as the bias/variance tradeoff (Briscoe & Feldman, 2011).-->

## Statistical underpinnings

To understand how BRMA estimates the relevant parameters and performs variable selection, it is instructional to first review the statistical underpinnings of the aforementioned classic approaches to meta-analysis.
First is the fixed-effect model, which assumes that each observed effect size $T_i$ is an estimate of an underlying true effect size $\Theta$ [@hedgesFixedRandomeffectsModels1998].
The only cause of heterogeneity in observed effect sizes is presumed to be effect size-specific sampling variance, $v_i$, which is treated as known, and computed as the square of the standard error of the effect size.
Thus, for a collection of $k$ studies, the observed effects sizes of individual studies $i$ (for $i$ = 1,2, . . . $k$) are given by:

\begin{align}
T_i &= \Theta + \epsilon_i\\
\text{where } \epsilon_i &\sim N(0, v_i)
\end{align}

<!--
\begin{aligned}
y_i &= \theta_i + \epsilon_i &\text{where } \epsilon_i \sim N(0, \sigma^2_i)\\ % \text{ and } 
%\theta_1 &= \theta_2 \dots = \theta_k
\end{aligned}-->

Under the fixed effect model, the estimated population effect size  $\hat{\theta}$ is obtained by computing a weighted average of the observed effect sizes.
If sampling error is assumed to be the only source of variance in observed effect size,
then it follows that studies with smaller standard errors estimate the underlying true effect size more precisely<!--(Schmidt, Oh \& Hayes, 2009)-->.
The fixed-effect weights are thus simply the reciprocal of the sampling variance, $w_{i} =  \frac{1}{v_i}$.
The estimate of the true effect is a weighted average across observed effect sizes:

\begin{equation}
\hat{\theta} = \frac{\sum_{i=1}^k w_iT_i}{\sum_{i=1}^k w_i}
\end{equation}
-->
<!--A highly likely consequence is that this will lead to a huge amount of possible moderators (Caserio, 2014). Caused by differences in for example cultures of research populations and used methods or instruments (Neuman, 2011). Even in replication studies there are sometimes moderators that are unanticipated (Kunert, 2016). This leads very often to an eventual poor performance of the fixed-effects meta-analysis model (Snijders, 2005). -->
	
Whereas the fixed-effect model assumes that only one true population effect exists,
the random-effects model assumes that true effects may vary for unknown reasons,
and thus follow a (normal) distribution of their own (Hedges \& Vevea, 1998).
This heterogeneity of the true effects is represented by their variance, $\tau^{2}$.
The random effect model thus assumes that the heterogeneity in observed effects can be decomposed into sampling error and between-studies heterogeneity, resulting in the following equation for the observed effect sizes:

\begin{align}
T_i &= \Theta + \zeta_{i} + \epsilon_i\\
\text{where } \zeta_i &\sim N(0, \tau^2)\\
\text{and } \epsilon_i &\sim N(0, v_i)
\end{align}

In this model, $\Theta$ is the mean of the distribution of true effect sizes, and $\tau^2$ is its variance, which can be interpreted as the variance between studies.

If the true effect sizes follow a distribution, 
then even less precise studies (with larger sampling errors) may provide some information about this distribution.
Like fixed-effect weights, random effects weights are still influenced by sampling error, but this influence is attenuated by the estimated variance of the true effect sizes. 
The random-effects weights are thus given by $w_{i} =  \frac{1}{v_i + \hat{\tau}^2}$.
It is important to note that,
whereas the sampling error for each individual effect size is treated as known,
between-study heterogeneity $\tau^{2}$ must be estimated.
This estimate is represented by $\hat{\tau}^{2}$.

### Meta-regression

The random effects model assumes that causes of heterogeneity in the true effect sizes are unknown, and that their influence is random.
Oftentimes, however, there are systematic sources of heterogeneity in true effect sizes.
These between-study differences can be coded as moderators, and their influence can be estimated and controlled for using meta-regression.
Meta-regression with $p$ moderators can be expressed with the following equation, where $x_{1\ldots p}$ represent the moderators, and $\beta_{1\ldots p}$ the regression coefficients:

\begin{align}
T_i &= \beta_{0}+ \beta_{1}x_{1}+ \beta_{2}x_{2} + \ldots + \beta_{p}x_{p} + \zeta_{i} + \epsilon_i\\
\end{align}

Note that $\beta_0$ represents the intercept of the distribution of true effect sizes after controlling for the moderators
and the error term $\zeta_{i}$ represents residual between-studies heterogeneity.
This term is still included because unexplained heterogeneity often remains after accounting for the moderators (Thompson \& Sharp, 1999).
This is a mixed-effects model; the intercept and effects of moderators are treated as fixed and the residual heterogeneity as random (Viechtbauer \& López-López, 2015).

<!--
An accurate estimation of the residual heterogeneity contributes to a better interpretation of the effect of the moderators (Panityakul, Bumrungsup \& Knapp, 2013).-->

To solve this model, the regression coefficients and residual heterogeneity must be estimated simultaneously.
<!--The topic of estimating the residual heterogeneity is a highly discussed one (Veroniki et al., 2016; Viechtbauer \& López-López, 2015; Panityakul et al., 2013). The ability of the estimators to predict the residual heterogeneity is influenced by different factors, such as the number of studies (Guolo \& Varin, 2017; Panityakul et al., 2013; Hardy \& Thompson, 1996) included and the sample size of the individual studies (Panityakul et al., 2013).
A third, and obvious factor, that is classified as relevant to model performance is heterogeneity among studies being meta-analysed (Kontopantelis \& Reeves, 2011; Jackson \& White, 2018). Coverage from models degrades when the residual heterogeneity increases, mostly when the amount of studies is small (Brockwell \& Gorden, 2001). Considering that all models their performance is linked to the accuracy of the estimate. According to Sidik \& Jonkman (2007), it is generally the case that the larger true between-study variance is, the more biased the estimate can be, which diminishes the performance of the method.-->
Numerous methods have been proposed to estimate meta-regression models, the most commonly used of which is restricted maximum likelihood (REML).
REML is an iterative method, meaning it performs the same calculations repeatedly, updating the estimated regression coefficients and residual heterogeneity until these estimates stabilize.
<!--, including the Hedges (HE), DerSimonian–Laird/Method of Moments (DL), Sidik and Jonkman (SJ), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), and Empirical Bayes (EB) method. These methods are mostly divided into two groups: closed-form or non-iterative methods and iterative methods. The main difference between these groups is that the closed form group uses a predetermined number of steps to provide an estimation for the residual heterogeneity, whereas the iterative methods run multiple iteration, as the name suggests, to converge to a solution when a specific criterion is met. It is important to note that some iterative methods do not produce a solution when they fail to converge after a predetermined amount of iteration.-->
In contrast to a regularized analysis technique, this estimator produces low bias, which means that the average value of the estimated regression coefficients and residual heterogeneity is close to their true values (Panityakul et al., 2013; Hardy & Thompson, 1996).
<!--It needs a starting estimation of $\tau^{2}$, which is usually estimated by one of the non-iterative methods (Viechtbauer \& López-López, 2015). Besides the starting value of $\tau^{2}$, it needs in every iteration an estimation of the regression coefficients of the moderators. These are typically estimated by using the Weighted Least Squares (WLS) method. This is a variation of the Ordinary Least Squares (OLS), but in the case of meta-analysis it is necessary to assess weights to the coefficients. In systematic reviews large variation in standard errors is often observed, which will result in large heteroscedasticity in the estimation of the effects (Stanley \& Doucouliagos, 2017). The addition of weights is a way to adjust for this heteroscedasticity. The weights are formulated as presented in equation (5).-->
<!--
The usage of a WLS method to estimate the regression coefficient may be problematic in the situation where a lot of moderators are measured without their specific effects, when the amount of studies is low and when moderators are dichotomous. The use of a least squares method will cause problems with the prediction accuracy and the model interpretability (James, Witten, Hastie, \& Tibshirani, 2013). In the situation where a lot of moderators are measured and blindly included in the model, it may as well be the case that variables are included that are in fact not associated with the response. Including irrelevant variables in the model lowers the interpretability of the model (James et al., 2013). An approach is necessary that automatically excludes the variables that are irrelevant i.e. performs variable selection. As explained before, in meta-analysis it is often the case that the number of moderators closely approaches or even exceeds the number of studies included in the analysis. A least squares method will display a lot variability in the fit when the number of variables is not much smaller than the number of studies (James et al., 2013). This means that the least squares method over fits the data and loses its power to be generalizable to future observations. When the number of variables exceeds the number of studies, the least squares method fails to produce one unique estimate and the method should not be used at all. 

However, a least squares method could still be somewhat valuable in some situations. It is extremely suitable to estimate a linear relationship. In the case of dichotomous moderators, the relationship is always perfectly linear. A powerful non-linear estimation tool is in the situation of dichotomous moderators unnecessary and would not perform better at all. Whenever a non-linear relation gets fitted on data with an underlying linear relation, it will cause problems when this fit gets used for the prediction of future data. Given the various arguments, this paper provides an approach to tackle this problem of the least squares methods whilst still making use of a linear method. The weighted least squares are replaced with the so-called LASSO regression for the estimation of the regression coefficients. This algorithm shrinks or penalizes the regression coefficients and performs variable selection (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008).-->

### Regularized regression

Regularized regression biases parameter estimates towards zero by including a shrinkage penalty in the estimation process.
Before examining the Bayesian case, we will explain the principle using frequentist OLS regression as an example.
OLS regression estimates the model parameters by minimizing the Residual Sum of Squares (RSS) of the dependent variable, which is given by:

$$
RSS=\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2
$$
The resulting parameter estimates are those that give the best predictions of the dependent variable in the present dataset.
Penalized regression, by contrast, adds a penalty term to the quantity to be minimized.
One commonly used penalty is the L1-norm of the regression coefficients, or LASSO penalty,
which corresponds to the sum of their absolute values.
This gives the penalized residual sum of squares:

$$
PRSS= RSS + \lambda \sum_{j=1}^{p}|\beta_{j}|
$$

Where $\lambda$ is a tuning parameter that determines how influential the penalty term will be.
If $\lambda$ is zero, the shrinkage penalty has no impact at all and the penalized regression will produce the OLS estimates.
If $\lambda \to \infty$, all coefficient shrink towards zero, producing the null model.
<!--Another commonly used penalty is the L2-norm of the coefficients, or ridge penalty,
which corresponds to the sum of their squared values: $\sum_{j=1}^{p}\beta_{j}^2$.-->
Because the penalty term is a function of the regression coefficients,
the optimizer has an incentive to keep the regression coefficients as small as possible.
<!--The LASSO penalty has the unique property of shrinking some coefficients to be exactly zero,
but the ridge penalty does not do so, as -->
Note that theLASSO penalty is but one example of a shrinkage penalty; other penalties exist, with some unique properties.

<!--The Lasso shrinkage method is not the only shrinkage method, there do exist some others. Nevertheless, the lasso is in the case the best option. It possesses, as opposed to other methods, the ability to shrink the parameter not towards zero, but to be exactly zero (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). This means that the lasso can perform variable selection, something that is specifically aimed for in this study. -->



<!--
**Alternative to linear model: Tree Based models**
An alternative that can perform variable selection, are tree-based models. These kinds of models have numerous other advantages over linear models. Tree-based models can be used for any data type, are easy to represent visually, require little data preparation and got larger power than linear regressions when moderators exceed observations in quantity. They are also more flexible in handling moderator interactions and non-linearity. As a result of that, they are better in modelling the complicated nature of human behaviour (Earp \& Trafimow, 2015). Decision trees split from the top down and group data in so-called ‘sub-nodes’, in which the data’s aspects are most homogeneous. The goal is to split to get the sub-nodes as uniform as possible, which can be until fully homogenous groups, or if a pre-specified touchstone is reached. 
Still, singletree based models have some limitations. First of all, tree models are unstable, small fluctuations that are utilized to make the model have a possibility to lead to considerable alterations in the constructions of the tree (Dwyer \& Holte, 2007). Second, it has problems with seizing linearity, because it only makes ‘twofold splits’ (Steyerberg, 2019). At last, tree-based models are susceptible to overfitting (Hastie et al, 2009). 

There are also more complex tree-based models, known as random-forests, which surmount most of the disadvantages of singletree. This variant incorporates multiple decision trees, and combines results from those trees to create a single model with a more accurate estimate (Breiman, 2001). The essential idea behind is know as the ‘wisdom of crowds’, a large number of relatively uncorrelated trees operating as a group will outperform any of the individual elements. The somewhat low correlation between the models is fundamental, because uncorrelated models are able to produce ensemble predictions with a higher accuracy that any individual prediction. This is because the trees preserve each other from their own singular errors (Genuer, Poggi \& Tuleau-Malot, 2010). The lower tendency to overfitting is another advantage of random forests over single trees (Bühlmann \& Yu, 2002). As well as the possibility to predict cases that are not components of the bootstrap sample of the tree. This kind of measure is known as out-of-bag error, which is an approximation of the cross-validation error, and provides proper estimates of the prediction accuracy in further samples (Hastie et al., 2009). 
An alternative to explore heterogeneity in meta-analysis with a singletree-based method is MetaForest. A technique developed by van Lissa (2017), designed to overcome the lacking’s of singletrees by using random forests. MetaForest applies random-effects or fixed-effects weights to random forests.
Based on two simulation studies, van Lissa (2017) examined the performance of fixed-effects, random-effects and unweighted MetaForest.

The study displayed also other advantages from random forests over singletrees. It had greater power, was able to make better predictions, gave estimates of the cross-validation error and yielded useful measures of variable importance and partial prediction plots (van Lissa, 2017). MetaForest can at the moment be considered as the best working technique to explore heterogeneity in meta-analysis. In van Lissa (2017), that only presented estimates of $\tau^{2}$ based on the raw data, we saw that MetaForest had certain robustness against a low number of studies. If moderators were continuously distributed, MetaForest had sufficient power at approximately 20 studies. However, there is an important feature to prove before we can make such an assumption. The underlying data generating models in the two simulation studies of van Lissa (2017) only included normal distributed moderators. Renouncing from normal distributions may affect the performance of the model, but since normal distribution in real-life data is more an exception than a normal state of affairs (Micceri, 1989), it is entirely possible that procedures are affected by skewness, leverage, balance etc. It is important to know how MetaForest performs in these kinds of situations.-->

### Bayesian estimation

<!--To solve this model, the moderator coefficients and the residual heterogeneity must be estimated simulataneously.
<!--The topic of estimating the residual heterogeneity is a highly discussed one (Veroniki et al., 2016; Viechtbauer \& López-López, 2015; Panityakul et al., 2013). The ability of the estimators to predict the residual heterogeneity is influenced by different factors, such as the number of studies (Guolo \& Varin, 2017; Panityakul et al., 2013; Hardy \& Thompson, 1996) included and the sample size of the individual studies (Panityakul et al., 2013).
A third, and obvious factor, that is classified as relevant to model performance is heterogeneity among studies being meta-analysed (Kontopantelis \& Reeves, 2011; Jackson \& White, 2018). Coverage from models degrades when the residual heterogeneity increases, mostly when the amount of studies is small (Brockwell \& Gorden, 2001). Considering that all models their performance is linked to the accuracy of the estimate. According to Sidik \& Jonkman (2007), it is generally the case that the larger true between-study variance is, the more biased the estimate can be, which diminishes the performance of the method.-->
<!--Numerous methods have been proposed to accurately estimate the residual heterogeneity, including the Hedges (HE), DerSimonian–Laird/Method of Moments (DL), Sidik and Jonkman (SJ), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), and Empirical Bayes (EB) method. These methods are mostly divided into two groups: closed-form or non-iterative methods and iterative methods. The main difference between these groups is that the closed form group uses a predetermined number of steps to provide an estimation for the residual heterogeneity, whereas the iterative methods run multiple iteration, as the name suggests, to converge to a solution when a specific criterion is met. It is important to note that some iterative methods do not produce a solution when they fail to converge after a predetermined amount of iteration. 

In our scenario we are especially interested in an estimator which performs well under the condition of a relative low number of studies. The Restricted Maximum Likelihood (REML) seems to produce the lowest bias under this condition and is therefore preferred (Panityakul et al., 2013; Hardy & Thompson, 1996). The REML is an iterative method and needs a starting estimation of $\tau^{2}$ to start, usually it gets estimated by one of the non-iterative methods (Viechtbauer \& López-López, 2015). Besides the starting value of $\tau^{2}$, it needs in every iteration an estimation of the regression coefficients of the moderators. These are typically estimated by using the Weighted Least Squares (WLS) method. This is a variation of the Ordinary Least Squares (OLS), but in the case of meta-analysis it is necessary to assess weights to the coefficients. In systematic reviews large variation in standard errors is often observed, which will result in large heteroscedasticity in the estimation of the effects (Stanley \& Doucouliagos, 2017). The addition of weights is a way to adjust for this heteroscedasticity. The weights are formulated as presented in equation (5). 

The usage of a WLS method to estimate the regression coefficient may be problematic in the situation where a lot of moderators are measured without their specific effects, when the amount of studies is low and when moderators are dichotomous. The use of a least squares method will cause problems with the prediction accuracy and the model interpretability (James, Witten, Hastie, \& Tibshirani, 2013). In the situation where a lot of moderators are measured and blindly included in the model, it may as well be the case that variables are included that are in fact not associated with the response. Including irrelevant variables in the model lowers the interpretability of the model (James et al., 2013). An approach is necessary that automatically excludes the variables that are irrelevant i.e. performs variable selection. As explained before, in meta-analysis it is often the case that the number of moderators closely approaches or even exceeds the number of studies included in the analysis. A least squares method will display a lot variability in the fit when the number of variables is not much smaller than the number of studies (James et al., 2013). This means that the least squares method over fits the data and loses its power to be generalizable to future observations. When the number of variables exceeds the number of studies, the least squares method fails to produce one unique estimate and the method should not be used at all. 

However, a least squares method could still be somewhat valuable in some situations. It is extremely suitable to estimate a linear relationship. In the case of dichotomous moderators, the relationship is always perfectly linear. A powerful non-linear estimation tool is in the situation of dichotomous moderators unnecessary and would not perform better at all. Whenever a non-linear relation gets fitted on data with an underlying linear relation, it will cause problems when this fit gets used for the prediction of future data. Given the various arguments, this paper provides an approach to tackle this problem of the least squares methods whilst still making use of a linear method. The weighted least squares are replaced with the so-called LASSO regression for the estimation of the regression coefficients. This algorithm shrinks or penalizes the regression coefficients and performs variable selection (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). -->

<!--
**Intro rma**
The rma algorithm is part of the software-package `metafor` in `R`, which is developed by Wolfgang Viechtbauer (2010, 2019). This algorithm is specifically developed to perform a meta-analysis or meta-regression. It allows to include different models, such as the fixed-, random- and mixed-effect model. It is also possible to account for moderators (Viechtbauer, 2010). The mixed-effect model, which is used is this study, requires a two-step approach to fit a meta-analytic model. First the residual heterogeneity is estimated. The package developed by Viechtbauer does provide multiple methods for the estimation of the residual heterogeneity. In this study the Restricted Maximum-likelihood is used, but this has already been discussed earlier. The second step is estimating the moderator coefficients, which is done by using the Weighted Least Squares (WLS) method. The weights are described in equation (5). The lma is a variation of the rma algorithm which is created by Caspar van Lissa. As explained before, the REML is an iterative procedure for the estimation of the residual heterogeneity. In every step of the process, instead of estimating the coefficients of the moderators by using a WLS, a weighted lasso regression is performed. Then again, the residual heterogeneity gets estimated with the rma algorithm by using the new values of the coefficients. With these new values of $\tau^{2}$, a new weighted lasso is performed for the estimations of the coefficients. This process continuous, until the residual heterogeneity converges to a certain value. -->

# Simulation study

The present study set out to validate the BRMA algorithm using a simulation study.
As a benchmark for comparison, we used restricted maximum likelihood meta-regression,
which is the standard in the field.
The algorithms are evaluated on three different criteria:
The algorithms' predictive performance in new data, 
<!-- examining its predictive performance in new data, its sensitivity and specificity in variable selection, and its bias and variance in recovering population parameters. -->
their ability to perform variable selection,
and their ability to recover population parameters. 

## Performance indicators

Predictive performance reflects how well the algorithm is able to predict data not used to estimate the model parameters, in other words, it indicates the generalizability of the model.
To compute it, for each iteration of the simulation both a training dataset and a testing dataset are generated.
The model is estimated on the training data, which has a varying number of cases according to the simulation conditions.
Predictive performance is then operationalized as the explained variance in the testing data, $R^2_{test}$.
The testing data has 100 cases in all simulation conditions.
The $R^2_{test}$ reflects the fraction of variance in the testing data explained by the model,
relative to the variance explained by the mean of the training data.
For a predictive performance measure, it is necessary to use the mean of the training data, as the mean of the testing is a descriptive statistic of that sample.
The resulting metric $R_{test}^{2}$ is expressed by the following equation:

$$
R_{cv}^{2} = 1- \frac{\sum_{i=1}^{k}(y_{i-test}-\hat{y}_{i-test})^{2}}{\sum_{i=1}^{k}(y_{i-test}-\bar{y}_{train})^{2}}
$$

With $k$ being the number of studies in the testing dataset, $\hat{y}_{i-test}$ being the predicted effect size for study $i$, and $\bar{y}_{train}$ being the mean of the training dataset.

The algorithms' ability to perform variable selection was evaluated by three indices: sensitivity, specificity, and the fraction of the product over the sum (FPS).
These indices are based on proportions computed across replications within each condition.
Sensitivity $P$ is the ability to select true positives, or the probability that a variable is selected, $S = 1$, given that it has a non-zero population effect: $P = p(S = 1||\beta| >0)$.
Specificity is the ability to identify true negatives, or the probability that a variable is not selected given that it has a zero population effect: $N = p(S = 0|\beta  = 0)$.
It is possible for a poorly-fitted model to score highly on specificity by predicting all null effects while detecting no true positives.
Therefore, we additionally computed a the fraction of the product over the sum (FPS) as an overall performance criterion, which is given by $FPS=  2(\frac{P*N}{P+N})$.

The ability to recover population parameters $\beta$ and $\tau^2$ was examined in terms of bias and variance of these estimates.
The bias is given by the mean deviation of the estimate from the population value,
and the variance is given by the variance of this deviation.

## Design factors

The simulation conditions consist of all unique combinations of the following design factors: the number of studies in the training data $k \in (22, 40, 80)$, average within-study sample size $\bar{n} \in (40, 100, 200)$, the population effect size of relevant moderators $\beta \in (.2, .5, .8)$, the number of standard normally distributed moderators $p \in (2, 3, 6)$, and residual heterogeneity $\tau^{2} \in (.01, .04, .1)$.
The true effect size was simulated using two models:
one with a main effect of one moderator, $T_{i}= \beta x_{1i} + \epsilon_i$, and one with a non-linear, cubic effect of one moderator, $T_{i}= \beta x_{1i} + \beta x_{1i}^{2} + \beta x_{1i}^{3} + \epsilon_i$,
where $\epsilon_i \sim N(0, \tau^2)$.
The observed effect size $y_i$ was then simulated as a standardized mean difference (SMD),
sampled from a non-central t-distribution.
For all simulation conditions, 100 datasets were generated. 

# Results

There were 3888 condition combinations in total. The algorithms ran on 100 different datasets on each of those, leaving 388800 cases to analyze. There were 20 of those cases for which the RMA algorithm had missing values on all metrics. Closer inspection showed that both the cubic and exponential model each contributed ten times to the missing values and only when 2, 3 or 6 moderators were taken up in the model. However, since the missing cases make up only 0.005% of the data, the cases were chosen to be omitted from further analysis entirely. Another observation is that the two-way interaction model only had results when the number of moderators in the model was either 3, 4 or 7, while the other models only had results when there were 2, 3 or 6 moderators. It is therefor more challenging to compare and interpret the effect the moderators had on the performance criteria between the two-way interaction and the other models.

## Predictive performance
Predictive performance was operationalized by calculating the $R^2_{test}$ and $MSE_{test}$ for every combination of design factors, in further test denoted as $R^2$ and $MSE$ respectively. The densities for the $R^{2}_{test}$ and $MSE_{test}$ values were skewed however, which is why it was chosen to use the median $R^2$ as the metric for predictive performance, rather than the mean. The spread of the metrics was described using the Mean Absolute Deviation [MAD], rather than the standard deviation. It was found that the Horseshoe, Lasso and RMA algorithm performed similarly overall, $R^2_{Hs} = 0.51 \pm 0.36$, $MSE_{Hs} = 0.21 \pm 0.18$ ; $R^2_{Lasso} = 0.50 \pm 0.37$, $MSE_{Lasso} = 0.21\pm 0.19$; $R^2_{RMA} = 0.50 \pm 0.37$, $MSE_{RMA} = 0.22 \pm 0.23$ . The MetaForest algorithm performed worst on $R^2$: $R^2_{Mf} = 0.35 \pm 0.38$, $MSE_{Mf} = 0.22 \pm 0.19$ 

To determine the effect of the design factors on $R^2$ for all algorithms, four separate ANOVA’s were performed; one per algorithm. The effect size $\eta^{2}$ per condition per algorithm, including for all two-way interactions can be found in table 1. Do note that the ANOVA's were performed with the normality assumption violated. The estimates serve mostly as a guidance, rather than an absolute result. 

Not too surprisingly, It was found that the true effect size $\beta$ had the largest effect on $R^{2}$ for all algorithms. As $\beta$ increased, the performance of all algorithms increased as well. However, $\beta$ did interact with the model that was estimated, being either a linear, two-way interaction, cubic or an exponential model. A graphical representation of the interaction is shown in image 1. When the exponential and cubic model were estimated, the increase of $R^{2}$ slowed for every higher value of $\beta$. For the estimation of the two-way interaction model the increase only slowed when $\beta$ went from 0.5 to 0.8, while the steepest increase for the linear model estimation was when $\beta$ increased from 0.2 to 0.5. Also noteworthy is that $R^2$ stagnated during estimation of the cubic model as $\beta$ went up to 0.5, while for the other models $R^2$ did keep increasing. There was little difference in median $R^2$ between Horsehoe, Lasso and RMA, while MetaForest performed worst.

The second largest marginal effect was that of the estimated model. All algorithms had the highest $R^2$ under the cubic model, followed by a similar performance on the two-way interaction and exponential models. All algorithms performed worst for the linear model. There again was little difference in performance between the Pema algorithms and RMA, although MetaForest performed worst. Image 2A shows the relationship. 

There also was a moderate interaction effect between the estimated model and the amount of skewness of the input data $\alpha$, especially for the Pema algorithms. Again, Pema and RMA algorithms performed best, followed by MetaForest in all conditions. Most obvious to note is that all algorithms, except for when the linear model was estimated, generally performed better on more skewed data, although the algorithms did perform worse during estimation of the two-way interaction model when $\alpha$ went from 5 to 10. Overall performance was best during estimation of the cubic model, but the performance difference between estimated models decreased as $\alpha$ increased. Image 3 shows the relationship.

The true residual heterogeneity $\tau^{2}$ had a negative linear relationship for all algorithms on $R^{2}$. That is, as $\tau^{2}$ increased, $R^{2}$ decreased. Image 2B shows the relationship.

The mean sample size per study $\bar{n}$ also had a moderate effect. For all algorithms the effect of $\bar{n}$ was positively linearly related with $R^{2}$. Again, the Pema and RMA algorithms performed better than MetaForest. Image 2C shows the relationship.

An especially large effect was found for the number of studies used in the training data $\kappa$ for MetaForest, while this effect was substantially smaller for RMA, Lasso and Horseshoe. The relationship is positively linear for all algorithms, but the slope is especially steep for MetaForest. Image 2D shows the relationship.

Finally, the number of moderators did not have a big effect for the Pema algorithms, while for RMA and MetaForest the effect was more noticeable. The relationship is shown in image 7. The relationship is generally negative with more moderators meaning worse performance, although an increase can be observed as the number of moderators increase from 4 to 6 for all algorithms except MetaForest. This increase in performance for MetaForest appears when the number of moderators go from 3 to 4.

## Estimating residual heterogeneity
The ability of the algorithms to correctly estimate $\tau^{2}$ was operationalized by subtracting the true value for $\tau^{2}$ from the $\tau^{2}$ estimated by the algorithms. Again, the median and Mean Absolute Deviation were used as metrics for performance. The RMA algorithm showed the best results, $\Delta\tau^2_{RMA} = 0.02 \pm 0.06$, followed by the MetaForest algorithm $\Delta\tau^2_{Mf} = 0.09 \pm 0.13$. The Pema algorithms performed worst $\Delta\tau^2_{Lasso} = 0.23 \pm 0.18$; $\Delta\tau^2_{Hs} = 0.23 \pm 0.17$. The finding that all medians are positive implies that all algorithms have a bigger tendency to overestimate $\tau^{2}$ than underestimate it. One comment to make is that uncertainty of the estimates generally increased as $\Delta\tau^{2}$ also increased. This implies that there was more variation in performance as median performance worsened.

To determine the effect of the design factors on $\Delta\tau^{2}$ for all algorithms, four separate ANOVA’s were performed. The effect size $\eta^{2}$ per condition per algorithm, including $\eta^2$ for all two-way interactions can be found in table 2. Again, the assumption of normality was violated.

The biggest predictor on the correct estimation of $\tau^{2}$ was the estimated model. This was mainly because the algorithms overestimated $\tau^{2}$ most when the model contained cubic terms. Image 5A shows the marginal relationship of the estimated model on $\Delta\tau^{2}$ . It becomes more clear why this overestimation occurred when showing the interaction between $\beta$ and the model estimated on $\Delta\tau^{2}$, shown in image 6. First note the general trend that during estimation of all models $\tau^{2}$ got more overestimated as $\beta$ increased, except during estimation of the linear model, where the effect of $\beta$ on $\Delta\tau^{2}$ was close to zero, except for MetaForest. However, note the scales for the y-axes. While estimating the two-way interaction, linear and exponential model, $\Delta\tau^2$ stayed well within a confined interval. However, the algorithms severely overestimated $\tau^{2}$ when the model contained cubic terms. Especially MetaForest overestimated $\tau^{2}$ substantially when $\beta = 0.8$ and the estimated model is cubic: $\Delta\tau^{2}_{MF} = 2.92 \pm 2.37$. The other algorithms also had a $\Delta\tau^{2} > 1$ in these conditions, but the results were not as severe. Interestingly, the Pema algorithms even outperformed the RMA algorithm in these conditions. 

The marginal effects of $\beta$ on $\Delta\tau^{2}$ are shown in image 5B. MetaForest was affected most by the increase in $\beta$, but in general performed better than the Pema algorithms when $\beta < 0.8$. The RMA algorithm performed best overall.

The marginal effect of $\alpha$ on $\Delta\tau^{2}$ was rather minimal, although there was a slight decrease in $\Delta\tau^{2}$ as $\alpha$ increased. However, the decrease is more explicit when the interaction of $\alpha$ with the estimated model is added. Image 7 shows this interaction. The algorithms were rather unaffected by $\alpha$ for the linear model, and a small decrease in $\Delta\tau^{2}$ as $\alpha$ increased can be seen in the exponential model. When the two-way interaction model was estimated however, the algorithms benefitted as $\alpha$ increased, while for the cubic model, $\Delta\tau^{2}$ first increased as $\alpha$ increased from 0 to 2, but decreased as $\alpha$ increased from 2 to 10. RMA performed best, followed by MetaForest. The Pema algorithms performed similarly, but worst. 

The effect of the true $\tau^{2}$ on $\Delta\tau^{2}$ was rather unnoticeable for the RMA and MetaForest algorithms. The tendency for the Pema algorithms on the other hand, was to overestimate $\tau^{2}$ more as the true $\tau^{2}$ increased. Image 5C shows the marginal relationship.

The effect of the number of moderators on $\Delta\tau^{2}$  was not that large either. A small increase in $\Delta\tau^{2}$ can be seen in the RMA and MetaForest algorithm as the number of moderators increased which was not found for the Pema algorithms. However, A small note is that MetaForest did substantially increase in $\Delta\tau^{2}$ as more moderators were added and the estimated model is cubic. Image 8 shows the interaction between the number of moderators and the estimated model.

$\kappa$ only had a substantial effect for MetaForest; the $\Delta\tau^{2}$  decreased quite rapidly if $\kappa$ increased, especially when the cubic model was estimated. For the other algorithms, decreasing $\kappa$ had little to no effect on correctly estimating the residual heterogeneity. Image 9 shows the interaction of $\kappa$ with the estimated model.

Finally, the average number of observations in the studies did not have a substantial effect on $\Delta\tau^{2}$. Image 5D shows the marginal relationship.

## Variable selection
To determine the extent to which the algorithms could perform variable selection correctly, the proportion true positives [$TP$] and true negatives [$TN$] were calculated. The $TP$ and $TN$ reflect how well the algorithms accredit importance to relevant moderators and discredit importance to irrelevant moderators respectively. It should be noted that $TP$ could only take on values 1 or 0 per simulated iteration, because in all models where $\beta > 0$, only one moderator was simulated to be relevant. As $\beta$ increased, the already relevant moderator increased in relevance, rather than spreading the relevance over the other moderators. $TN$ had a bigger range and could take on values dependent on how many moderators were taken up in the model. E.g. when $\beta = 0$ and $n_{mods} = n$, $n+1$ different proportions were possible, $\frac{0}{n}$ up until $\frac{n}{n}$.

There were no differences in variables selected by Highest Density Intervals or Confidence Intervals for both Lasso and Horshoe and so for both algorithms it did not matter which interval type was analyzed. It was found that MetaForest had the highest proportion true positives: $TP_{Mf} = 0.98$, closely followed by RMA: $TP_{RMA} = 0.96$. Horshoe performed slightly better than Lasso;  $TP_{Hs} = 0.91$; $TP_{Lasso} = 0.89$. As for $TN$, it was found that the pema algorithms performed best: $TN_{Hs} \ \text{and} \ TN_{Lasso} = 0.93$, followed by RMA: $TN_{RMA} = 0.89$. MetaForest performed worst by a large margin: $TN_{Mf} = 0.50$. The Mean Absolute Deviation for all algorithms on both $TP$ and $TN$ was 0, except for MetaForests performance on $TN$, where the Mean Absolute Deviation was 0.44. 

Perfomance on $TP$ and $TN$ were very high for all algorithms, with all mean proportions, except MetaForests performance on $TN$, exceeding .89. This implies that MetaForest had issues excluding irrelevant moderators from the models. Plots were inspected to determine the effects of the design factors on the proportions. While inspecting the plots, there were found to be little marginal effects of the design factors on $TN$, while $TP$ was more affected. 

Firstly, $\kappa$ only had a positive effect on $TP$. As $\kappa$ increased, $TP$ also increased. MetaForest had the highest $TP$, followed by RMA and, lastly, Horsehoe and Lasso. The increase in $TP$ for higher values of $\kappa$ was steeper for the Pema algorithms, however. There was also an interaction of $\kappa$ with the estimated model shown in image 10. During estimation of the linear and two-way interaction model, the relationship of $\kappa$ looked relatively linear. At $\kappa = 20$ the $TP$ was relatively low for the algorithms, compared to the cubic model where $TP$ starts at .98 and converges to 1 as $\kappa$ increased. This latter relationship was also found for the exponential model, although the $TP$ at $\kappa = 20$ was lower.

$\bar{n}$ had a positive and roughly linear relationship with $TP$ for all algorithms. MetaForest performed best, followed by RMA, while Horseshoe and Lasso performed worst. Image 11A shows the relationship.

$\beta$ had an interaction effect with the model estimated on $TN$. Only during estimation of the two-way interaction model, $TN$ decreased as $\beta$ increased, For the other models, $TN$ remained stable. This could be because the interaction model was only fitted when there were 3,4 or 7 moderators, while for the other models, only 2,3 or 6 moderators were used. Image 12 shows the relationship. The effect of $\beta$ on $TP$ was positive; as $\beta$ increased, $TP$ increased too.

The true $\tau^2$ had a negative effect on $TP$, while $\alpha$ seemed to have little effect on both $TP$ and $TN$. Image 11B shows the marginal relationship of $\tau^2$ on $TP$.

Finally, there was an effect of number of moderators on $TN$, but only for the two-way interaction model. The $TN$ increased as the number of moderators did. Image 13 shows the interaction. This relationship was reversed for $TP$ and was found during estimation of all models, i.e. $TP$ decreased as the number of moderators increased. Image 14 shows the relationship.

# Discussion





\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
