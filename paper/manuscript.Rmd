---
title             : "Select relevant moderators using Bayesian regularized meta-regression"
shorttitle        : "BAYESIAN REGULARIZED META-REGRESSION"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1,2"
    corresponding: yes
    address: "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email: "c.j.vanlissa@uu.nl"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Funding acquisition
      - Methodology
      - Project administration
      - Software
      - Supervision
      - Writing – original draft
      - Writing – review & editing
  - name: "Sara van Erp"
    affiliation: "1"
    role:
      - Methodology
      - Software
      - Writing – original draft
      - Writing – review & editing
  - name: "Eli-Boaz Clapper"
    affiliation: "1"
    role:
      - Formal Analysis
      - Writing – original draft
      - Writing – review & editing
affiliation:
  - id            : "1"
    institution   : "Utrecht University, dept. Methodology & Statistics"
  - id            : "2"
    institution   : "Open Science Community Utrecht"

authornote: |
  This is a preprint paper, generated from Git Commit # `r substr(gert::git_commit_id(),1,7)`. This work was funded by a NWO Veni Grant (NWO Grant Number VI.Veni.191G.090), awarded to the lead author.

abstract: |
  When analyzing a heterogeneous body of literature, there may be many
  potentially relevant between-studies differences. These differences can be
  coded as moderators, and accounted for using meta-regression. However, many
  applied meta-analyses lack the power to adequately account for multiple
  moderators, as the number of studies on any given topic is often low.
  [The present study introduces Bayesian Regularized Meta-Analysis (BRMA),
  which uses regularizing (LASSO or horseshoe) priors to shrink small
  regression coefficients towards zero]{#lassoabstract}, thereby selecting relevant
  moderators from a larger number of candidates.
  This approach is suitable when heterogeneity is suspected,
  but it is not known which moderators most strongly inﬂuence the
  observed effect size.
  We present a simulation study to validate the performance of BRMA
  relative to state-of-the-art random effects meta-analysis using REML (RMA).
  Results indicated that BRMA compared favorably to RMA on three metrics:
  predictive performance (a measure of generalizability),
  the ability to reject irrelevant moderators,
  and the ability to recover population parameters with
  low bias.
  BRMA had slightly lower ability to detect true effects of relevant
  moderators, but the overall proportion of Type I and Type II errors was
  equivalent to RMA. BRMA regression coefficients were slightly
  biased towards zero (by design), but its estimates of residual heterogeneity
  were unbiased. BRMA performed well with as few as 20 studies in the training
  data, suggesting its suitability as a small sample solution. We discuss how
  applied researchers can use BRMA to explore between-studies heterogeneity
  in meta-analysis.
  The method is implemented in the R-package 
  `pema` (penalized meta-analysis) and in the free open source statistical software package JASP.
  
keywords          : "meta-analysis, machine learning, bayesian, lasso, horseshoe, regularized"
wordcount         : "5356"

bibliography      : ["r-references.bib", "references.bib",
                     "sara-references.bib", "eli-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
knit: worcs::cite_essential
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
run_everything <- FALSE
library("papaja")
library(tidySEM)
library(kableExtra)
r_refs("r-references.bib")
out <- readRDS("output.RData")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, warning = FALSE)
```

<!--
Skeleton lasso/pema paper
1.) What is Meta-analysis?
2.) What is meta-regression and how does it complement meta-analysis?
	a.) Introduce Moderators
	b.) Study Heterogeneity + Random Sampling Error (and their difference)	
2.5.) Fixed vs. random effects
	a.) Shortcomings of fixed effect models
3.) Shortcomings of current meta regressions w.r.t. estimating coefficients and heterogeneity:
	a.) Small sample size / overfitting
	b.) Non-normal data
4.) Various methods to estimate heterogeneity (and coefficients)
	a.) The use of WLS and REML
5.) Intro to Frequentist linear methods/Bayesian methods and Random forests, along with their 
(dis-)advantages:
	a.) Rma: uses WLS for estimation
	b.) MetaForest (Random Effects): Uses Random Forest Algorithm
	c.) Lasso Pema: uses penalized Lasso
	d.) Horseshoe Pema: uses horseshoe priors
6.) Goal of the current study
7.) Means of attaining goal and evaluation of performance:
	a.) simulation study
	b.) algorithmic performance
	c.) design factors
	d.) Impact of design factors on algorithim performance
	e.) Hypotheses of algorithmic performances

colour coding: I colour coded the text as to know from which file the text is copied
GREEN = derived from ‘Thesis_Metaforest’
BLUE = Thesis_lasso
BLACK = internship_report
RED = Inserted myself

# Introduction
--> 

Meta-analysis is a quantitative form of evidence synthesis,
whereby effect sizes from multiple similar studies are aggregated.
In its simplest form, this aggregation consists of a weighted average of the observed effect sizes.
Weighting accounts for the fact that some observed effect sizes are assumed to be more informative about the underlying population effect.
<!-- Each effect size is assigned a weight that determines how influential it is in calculating the summary effect. -->
The weights are based on specific assumptions;
for example, the *fixed effect* model assumes that all observed effect sizes reflect one underlying true population effect.
This assumption is appropriate when meta-analyzing effect sizes from close replication studies [@higginsReevaluationRandomeffectsMetaanalysis2009]. 
The *random effects* model, by contrast, assumes that population effect sizes follow a normal distribution.
<!-- Each observed effect size provides information about the mean and standard deviation of this distribution. -->
This assumption is more appropriate when studies are conceptually similar but vary in small random ways that introduce heterogeneity in effect sizes [@higginsReevaluationRandomeffectsMetaanalysis2009]. 

Heterogeneity in effect sizes is not always random, however.
When similar research questions are studied by different labs, in different populations, using different study designs, measurement instruments, and methods - those between-study differences may introduce *systematic heterogeneity*.
Suspected causes of systematic heterogeneity can either be used as exclusion criteria,
or accounted for using *meta-regression* [see @lopez-lopezEstimationPredictivePower2014].
Meta-regression estimates the effect of study characteristics on effect size,
and provides an estimate of the overall effect size and residual heterogeneity after controlling for their influence.
For example, if studies have been replicated in Europe and the Americas,
one could either exclude studies from Europe from further analysis,
or code a binary moderator variable called "continent".
One could then estimate the average effect size across both continents.
Similarly, if studies have examined the effect of a drug at different dosages,
one could code dosage as a continuous moderator, and control for its influence - thus estimating the overall effect size at average dosages.

A common application of meta-analysis is to summarize existing bodies of literature.
In such situations, there are many potentially relevant between-study differences that could be coded as moderators.
Although meta-regression can accommodate multiple moderators,
like any regression-based approach, it requires a relatively high number of cases (studies) per parameter to obtain sufficient statistical power.
In applied meta-analyses, the number of available studies is often too low to obtain sufficient power [@rileyInterpretationRandomEffects2011],
or even so low that the model is not identified [@akaikeNewLookStatistical1974].
This problem is known as the "curse of dimensionality".
<!-- This poses a challenge in the -->
<!-- classic meta-analysis paradigm, which, like any regression-based approach, requires a -->
<!-- high number of cases per parameter.  -->
Between-studies differences thus pose a non-trivial challenge to classic meta-analytic methods.
At the same time, they also provide an unexploited
opportunity to learn which factors impact the effect size
found, if adequate exploratory techniques are used. 

The curse of dimensionality can be overcome using *variable selection*:
identifying a smaller subset of relevant moderators from a larger number of candidate moderators [@hastieElementsStatisticalLearning2009].
One way to perform variable selection is by relying on theory, and selecting only moderators that should theoretically have an impact on effect size.
Note, however, that theories at the individual level of analysis do not necessarily generalize to the study level of analysis.
In the social sciences, for example, many theories describe phenomena at the level of individual people.
Using such theories for variable selection at the study level amounts to committing the ecological fallacy: generalizing inferences across levels of analysis [@jargowskyEcologicalFallacy2004].
To illustrate what a theory at the study level of analysis might look like, 
consider the so-called *decline effect*.
It is a phenomenon whereby effect sizes in a particular tranche of the literature seem to diminish over time [@schoolerUnpublishedResultsHide2011].
It has been theorized that the decline effect can be attributed to regression to the mean:
A finding initially draws attention from the research community because an anomalously large effect size has been published, and subsequent replications find smaller effect sizes.
Based on the decline effect, we might thus expect the variable "year of publication" to be a relevant moderator of study effect sizes.
Note that this prediction is valid even if year is orthogonal to the outcome of interest within each study.
Until more theory about the drivers of between-study heterogeneity is developed,
however, this approach will have limited utility for variable selection.
<!--A second example that might illustrate this problem is a study that concluded that American college students' empathy had decreased over time, based on meta-analysis of 72 samples collected between 1979 and 2009.-->

An alternative solution is to rely on statistical methods for variable selection.
This is a focal issue in the discipline of machine learning [@hastieElementsStatisticalLearning2009].
There is precedent for the use of machine learning to perform variable selection in meta-analysis [@vanlissaSmallSampleMetaanalyses2020].
This prior work used the non-parametric *random forest* algorithm.
One limitation of random forests is that non-parametric models are harder to interpret,
particularly when the readership is accustomed to linear models that describe the effect of each moderator with a single parameter.

Regularization is a method for variable selection that can be applied in linear models.
It effectively shrinks small model parameters towards zero,
such that irrelevant moderators are eliminated.
The present paper introduces *Bayesian regularized meta-regression* (BRMA),
an algorithm that uses Bayesian estimation with regularizing priors to perform variable selection in meta-analysis.
Regularizing priors assign a high probability mass to near-zero values,
which keeps small regression coefficients close to zero, resulting in a sparse solution.
This manuscript discusses two shrinkage priors,
the LASSO and horseshoe prior.
<!--
Thus, there is a need of a regularization method to curtail overfitting. Least Absolute Shrinkage and Selection Operator [LASSO] (L1-norm regularization) can fulfill this role, since it has an advantage in terms of feature selection. The goal of this project is to implement L1-norm regularization in the weighted meta-regression, developing an new estimator for regularized meta-regression.
This dilemma could be resolved by an exploratory technique that can identify relevant moderators in meta-analysis,
and which is relatively robust to small sample sizes.
-->
<!--Such shrinkage is advantageous because it performs variable selection and results in a sparse model.
Sparse models are easier to interpret, as fewer non-zero effects remain.
Regularization also provides a solution to the curse of dimensionality; unlike standard regression models, it can provide identified models even if the number of moderators exceeds the number of cases.-->
<!--By doing this it automatically performs variable selection. It does not seem to be immediately clear why shrinking the coefficients should be an improvement to the model. However, by shrinking the parameters, it lowers the variance of the model by increasing the bias only a little bit. In other words, the model sacrifices some of its ability to fit the current data, to greatly increase the ability to predict future data with the same fit (James et al., 2013). This is better known as the bias/variance tradeoff (Briscoe & Feldman, 2011).-->

## Statistical underpinnings

To understand how BRMA estimates the relevant parameters and performs variable selection,
it is instructional to first review the statistical underpinnings of classical meta-analysis.
As mentioned before, the fixed effect model assumes that each observed effect size $T_i$ is an estimate of an underlying true effect size $\Theta$ [@hedgesFixedRandomeffectsModels1998].
The only cause of heterogeneity in observed effect sizes is presumed to be sampling error, $v_i$, which is treated as known, and computed as the square of the standard error of the effect size.
Thus, for a collection of $k$ studies,
the observed effects sizes of individual studies $i$ (for $i \in [1,2, \ldots k]$) are given by:

\begin{align}
T_i &= \Theta + \epsilon_i\\
\text{where } \epsilon_i &\sim N(0, v_i)
\end{align}

The estimated population effect size $\hat{\theta}$ is then a weighted average of the observed effect sizes.
The assumption that sampling error is the only source of variance in observed effect sizes implies that studies with smaller standard errors estimate the underlying true effect size more precisely and should accrue more weight.
Therefore, fixed effect weights are simply the reciprocal of the sampling variance, $w_{i} =  \frac{1}{v_i}$.
The estimate of the true effect is a weighted average across observed effect sizes:

\begin{equation}
\hat{\theta} = \frac{\sum_{i=1}^k w_iT_i}{\sum_{i=1}^k w_i}
\end{equation}

<!--A highly likely consequence is that this will lead to a huge amount of possible moderators (Caserio, 2014). Caused by differences in for example cultures of research populations and used methods or instruments (Neuman, 2011). Even in replication studies there are sometimes moderators that are unanticipated (Kunert, 2016). This leads very often to an eventual poor performance of the fixed effects meta-analysis model (Snijders, 2005). -->
	
The random effects model assumes that, in addition to sampling error, true effects may vary for random reasons,
and thus follow a (normal) distribution with mean $\Theta$ and variance $\tau^{2}$ [@hedgesFixedRandomeffectsModels1998].
The observed effect sizes are thus given by:

\begin{align}
T_i &= \Theta + \zeta_{i} + \epsilon_i\\
\text{where } \zeta_i &\sim N(0, \tau^2)\\
\text{and } \epsilon_i &\sim N(0, v_i)
\end{align}

As in the fixed effect model,
studies with smaller sampling errors are assigned more weight.
However, to account for the fact that all studies now provide some information about different regions of the distribution of true effect sizes,
the weights are attenuated in proportion to the spread of that distribution.
The random effects weights are thus given by $w_{i} =  \frac{1}{v_i + \hat{\tau}^2}$.
Whereas sampling error is still treated as known,
the between-study heterogeneity $\tau^{2}$ must be estimated.
This estimate is represented by $\hat{\tau}^{2}$.

Meta-regression extends the random effects model to account for systematic sources of heterogeneity, which are coded as moderators.
The equation below describes a model with $p$ moderators, where $x_{1\ldots p}$ represent the moderators, and $\beta_{1\ldots p}$ their regression coefficients.
Note that $\beta_0$ represents the intercept of the distribution of true effect sizes after controlling for the moderators
and the error term $\zeta_{i}$ represents residual unexplained between-studies heterogeneity.
This is a mixed-effects model; the intercept and effects of moderators are treated as fixed and the residual heterogeneity as random [@lopez-lopezEstimationPredictivePower2014]:

\begin{align}
T_i &= \beta_{0}+ \beta_{1}x_{1}+ \beta_{2}x_{2} + \ldots + \beta_{p}x_{p} + \zeta_{i} + \epsilon_i\\
\end{align}

<!--The topic of estimating the residual heterogeneity is a highly discussed one (Veroniki et al., 2016; Viechtbauer \& López-López, 2015; Panityakul et al., 2013). The ability of the estimators to predict the residual heterogeneity is influenced by different factors, such as the number of studies (Guolo \& Varin, 2017; Panityakul et al., 2013; Hardy \& Thompson, 1996) included and the sample size of the individual studies (Panityakul et al., 2013).
A third, and obvious factor, that is classified as relevant to model performance is heterogeneity among studies being meta-analysed (Kontopantelis \& Reeves, 2011; Jackson \& White, 2018). Coverage from models degrades when the residual heterogeneity increases, mostly when the amount of studies is small (Brockwell \& Gorden, 2001). Considering that all models their performance is linked to the accuracy of the estimate. According to Sidik \& Jonkman (2007), it is generally the case that the larger true between-study variance is, the more biased the estimate can be, which diminishes the performance of the method.-->
The parameters of this model are the regression coefficients and residual heterogeneity.
Numerous methods have been proposed to estimate meta-regression models,
the most common of which is restricted maximum likelihood (REML).
REML is an iterative method, which means that it repeatedly performs the same calculations and updates the estimated parameters until their estimates stabilize.
<!--, including the Hedges (HE), DerSimonian–Laird/Method of Moments (DL), Sidik and Jonkman (SJ), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), and Empirical Bayes (EB) method. These methods are mostly divided into two groups: closed-form or non-iterative methods and iterative methods. The main difference between these groups is that the closed form group uses a predetermined number of steps to provide an estimation for the residual heterogeneity, whereas the iterative methods run multiple iteration, as the name suggests, to converge to a solution when a specific criterion is met. It is important to note that some iterative methods do not produce a solution when they fail to converge after a predetermined amount of iteration.-->
This estimator has low bias, which means that the average values of the parameters are close to their true values [@panityakulEstimatingResidualHeterogeneity2013].
<!-- ; (Panityakul et al., 2013; Hardy & Thompson, 1996). -->
However, this bias comes at the cost of higher variance, which means that the estimated values of a population parameter vary more from one sample to the next. 
An estimator with low bias and high variance produces results that generalize less well to new data than an estimator with high bias and low variance.
This phenomenon is known as the *bias-variance trade-off*.
Regularization increases bias to reduce variance.
A disadvantage of this trade-off is that model parameters can no longer be interpreted as straightforwardly as OLS regression coefficients.
An advantage is that the resulting model is more generalizable and makes better predictions for new data [see @hastieElementsStatisticalLearning2009].

<!--It needs a starting estimation of $\tau^{2}$, which is usually estimated by one of the non-iterative methods (Viechtbauer \& López-López, 2015). Besides the starting value of $\tau^{2}$, it needs in every iteration an estimation of the regression coefficients of the moderators. These are typically estimated by using the Weighted Least Squares (WLS) method. This is a variation of the Ordinary Least Squares (OLS), but in the case of meta-analysis it is necessary to assess weights to the coefficients. In systematic reviews large variation in standard errors is often observed, which will result in large heteroscedasticity in the estimation of the effects (Stanley \& Doucouliagos, 2017). The addition of weights is a way to adjust for this heteroscedasticity. The weights are formulated as presented in equation (5).-->
<!--
The usage of a WLS method to estimate the regression coefficient may be problematic in the situation where a lot of moderators are measured without their specific effects, when the amount of studies is low and when moderators are dichotomous. The use of a least squares method will cause problems with the prediction accuracy and the model interpretability (James, Witten, Hastie, \& Tibshirani, 2013). In the situation where a lot of moderators are measured and blindly included in the model, it may as well be the case that variables are included that are in fact not associated with the response. Including irrelevant variables in the model lowers the interpretability of the model (James et al., 2013). An approach is necessary that automatically excludes the variables that are irrelevant i.e. performs variable selection. As explained before, in meta-analysis it is often the case that the number of moderators closely approaches or even exceeds the number of studies included in the analysis. A least squares method will display a lot variability in the fit when the number of variables is not much smaller than the number of studies (James et al., 2013). This means that the least squares method over fits the data and loses its power to be generalizable to future observations. When the number of variables exceeds the number of studies, the least squares method fails to produce one unique estimate and the method should not be used at all. 

However, a least squares method could still be somewhat valuable in some situations. It is extremely suitable to estimate a linear relationship. In the case of dichotomous moderators, the relationship is always perfectly linear. A powerful non-linear estimation tool is in the situation of dichotomous moderators unnecessary and would not perform better at all. Whenever a non-linear relation gets fitted on data with an underlying linear relation, it will cause problems when this fit gets used for the prediction of future data. Given the various arguments, this paper provides an approach to tackle this problem of the least squares methods whilst still making use of a linear method. The weighted least squares are replaced with the so-called LASSO regression for the estimation of the regression coefficients. This algorithm shrinks or penalizes the regression coefficients and performs variable selection (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008).-->

### Regularized regression

Ordinary least squares regression (OLS) estimates the model parameters by minimizing the Residual Sum of Squares (RSS) of the dependent variable.
The resulting parameter estimates perfectly describe linear relations in the present data set, but generalize less well to new data:

$$
RSS=\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2
$$

Regularized regression biases parameter estimates towards zero by adding a penalty term to the RSS.
As an example, we will discuss the most common penalty: the sum of the absolute regression coefficients, also known as the the L1- or LASSO penalty [@hastieElementsStatisticalLearning2009].
Note that other penalties exist.
As the LASSO penalty is a function of the regression coefficients, it increases when they get bigger.
This incentivizes the optimizer to keep the regression coefficients as small as possible.
Note that the amount of regularization can be controlled by multiplying the penalty by a tuning parameter, $\lambda$.
If $\lambda$ is zero, the shrinkage penalty has no impact.
If $\lambda \to \infty$,
all coefficients shrink towards zero, producing the null model.
<!--Another commonly used penalty is the L2-norm of the coefficients, or ridge penalty,
which corresponds to the sum of their squared values: $\sum_{j=1}^{p}\beta_{j}^2$.-->
Cross-validation is often used to find the optimal value for the penalty parameter $\lambda$.
<!--The LASSO penalty has the unique property of shrinking some coefficients to be exactly zero,
but the ridge penalty does not do so, as -->
The LASSO penalized residual sum of squares is given by:

$$
PRSS= RSS + \lambda \sum_{j=1}^{p}|\beta_{j}|
$$

<!--The Lasso shrinkage method is not the only shrinkage method, there do exist some others. Nevertheless, the lasso is in the case the best option. It possesses, as opposed to other methods, the ability to shrink the parameter not towards zero, but to be exactly zero (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). This means that the lasso can perform variable selection, something that is specifically aimed for in this study. -->
<!--
**Alternative to linear model: Tree Based models**
An alternative that can perform variable selection, are tree-based models. These kinds of models have numerous other advantages over linear models. Tree-based models can be used for any data type, are easy to represent visually, require little data preparation and got larger power than linear regressions when moderators exceed observations in quantity. They are also more flexible in handling moderator interactions and non-linearity. As a result of that, they are better in modelling the complicated nature of human behaviour (Earp \& Trafimow, 2015). Decision trees split from the top down and group data in so-called ‘sub-nodes’, in which the data’s aspects are most homogeneous. The goal is to split to get the sub-nodes as uniform as possible, which can be until fully homogenous groups, or if a pre-specified touchstone is reached. 
Still, singletree based models have some limitations. First of all, tree models are unstable, small fluctuations that are utilized to make the model have a possibility to lead to considerable alterations in the constructions of the tree (Dwyer \& Holte, 2007). Second, it has problems with seizing linearity, because it only makes ‘twofold splits’ (Steyerberg, 2019). At last, tree-based models are susceptible to overfitting (Hastie et al, 2009). 

There are also more complex tree-based models, known as random-forests, which surmount most of the disadvantages of singletree. This variant incorporates multiple decision trees, and combines results from those trees to create a single model with a more accurate estimate (Breiman, 2001). The essential idea behind is know as the ‘wisdom of crowds’, a large number of relatively uncorrelated trees operating as a group will outperform any of the individual elements. The somewhat low correlation between the models is fundamental, because uncorrelated models are able to produce ensemble predictions with a higher accuracy that any individual prediction. This is because the trees preserve each other from their own singular errors (Genuer, Poggi \& Tuleau-Malot, 2010). The lower tendency to overfitting is another advantage of random forests over single trees (Bühlmann \& Yu, 2002). As well as the possibility to predict cases that are not components of the bootstrap sample of the tree. This kind of measure is known as out-of-bag error, which is an approximation of the cross-validation error, and provides proper estimates of the prediction accuracy in further samples (Hastie et al., 2009). 
An alternative to explore heterogeneity in meta-analysis with a singletree-based method is MetaForest. A technique developed by van Lissa (2017), designed to overcome the lacking’s of singletrees by using random forests. MetaForest applies random effects or fixed effects weights to random forests.
Based on two simulation studies, van Lissa (2017) examined the performance of fixed effects, random effects and unweighted MetaForest.

The study displayed also other advantages from random forests over singletrees. It had greater power, was able to make better predictions, gave estimates of the cross-validation error and yielded useful measures of variable importance and partial prediction plots (van Lissa, 2017). MetaForest can at the moment be considered as the best working technique to explore heterogeneity in meta-analysis. In van Lissa (2017), that only presented estimates of $\tau^{2}$ based on the raw data, we saw that MetaForest had certain robustness against a low number of studies. If moderators were continuously distributed, MetaForest had sufficient power at approximately 20 studies. However, there is an important feature to prove before we can make such an assumption. The underlying data generating models in the two simulation studies of van Lissa (2017) only included normal distributed moderators. Renouncing from normal distributions may affect the performance of the model, but since normal distribution in real-life data is more an exception than a normal state of affairs (Micceri, 1989), it is entirely possible that procedures are affected by skewness, leverage, balance etc. It is important to know how MetaForest performs in these kinds of situations.-->

### Bayesian estimation

An alternative to the use of a shrinkage penalty is Bayesian estimation with a regularizing prior.
Bayesian estimation combines information from the data with a *prior distribution*.
The prior distribution assigns a-priori probability to different parameter values.
Likely parameter values have a high probability density, and unlikely parameter values have a low probability density.
The aforementioned (frequentist) approaches, by contrast, treat every possible parameter value as equally plausible.
The prior distribution is updated with the likelihood of the data to form a posterior distribution,
which reflects expectations about likely parameter values after having seen the data [for an extensive introduction, see @mcelreathStatisticalRethinkingBayesian2020].

A regularizing prior distribution shrinks small coefficients towards zero by assigning high probability mass to near-zero values.
There are many different regularizing prior distributions [@vanErpOberskiMulder2019].
Some of these regularizing priors are analogous to specific frequentist methods.
For example, a double exponential prior (hereafter: LASSO prior) results in posterior distributions whose modes are identical to the estimates from LASSO-penalized regression [@ParkCasella2008].

A limitation of the LASSO prior is that it introduces substantial bias in non-zero regression coefficients.
To overcome this limitation, regularizing priors with better shrinkage properties have been developed.
These priors still pull small regression coefficients towards zero,
but exert less bias on larger regression coefficients.
One example is the horseshoe prior [@CarvalhoPolsonScott2010].
It has heavier tails than the LASSO prior,
which means that it does not shrink (and therefore bias) substantial coefficients as much.

The BRMA method introduced here offers both LASSO and horseshoe priors.
The LASSO prior is given by:
$$
\beta_{j} \sim \text{DE}(0, \frac{s}{\lambda})
$$
where DE denotes the double exponential distribution with a location equal to 0 and a scale determined by a global scale parameter $s$ and an inverse-tuning parameter $\lambda$.
In the present study, the global scale parameter is set to 1,
and the inverse-tuning parameter is assigned a $\chi^2$ prior distribution with 1 degree of freedom.
Its value is thus optimized during model estimation.

The regularizing horseshoe prior was proposed by @PiironenVehtari2017b and is given by:
$$
\begin{aligned}
\beta_{j} &\sim N(0, \tilde{\tau}_j^2\lambda) \text{, with } \tilde{\tau}_j^2 = \frac{c^2 \tau_j^2}{c^2 + \lambda^2 \tau_j^2}\\
\lambda &\sim \text{student-}t^+(\nu_1, 0, \lambda^2_0)\\
\tau_j &\sim \text{student-}t^+(\nu_2, 0, 1)\\
c^2 &\sim \Gamma^{-1}(\frac{\nu_3}{2}, \frac{\nu_3 s^2}{2})
\end{aligned}
$$
where $N$ denotes the normal distribution, student-$t^+$ denotes the half-t distribution and $\Gamma^{-1}$ denotes the inverse Gamma distribution. 
In this formula, $\lambda^2_0$ is a global scale parameter that affects the overall shrinkage of the prior, with smaller values resulting in more shrinkage.
In the present study, we assume a default value of $\lambda^2_0 = 1$.
However, if prior information regarding the expected number of relevant moderators is available,
it is best to include this information.
This is accomplished by setting $\lambda^2_0 = \frac{p_0}{p-p_0}{\frac{\sigma}{\sqrt{n}}}$, where $p_0$ represents the expected number of relevant moderators, $p$ the total number of moderators, $\sigma$ is the residual standard deviation and $n$ equals the number of observations.
<!-- SvE: nu twijfel ik toch weer of dit helemaal goed gaat. sigma is in ons model namelijk op 0 gezet en wordt hier dus niet in meegenomen. Klopt het dat de residual SD 0 moet zijn in dit model? De rest van de global scale wordt berekend buiten het Stan model om in de brma functie zelf. -->
The thickness of the tails is controlled by two degrees of freedom parameters, $\nu_1$ and $\nu_2$.
In this study, we assume default values of 1 for these parameters.
Increasing these degrees of freedom parameters results in a prior with lighter tails,
which is, strictly speaking, no longer a horseshoe prior.
However, in cases where the model is weakly identified, for example when there are more moderators than observations, these lighter tails can aid model convergence. 
The regularizing horseshoe differs from the standard horseshoe in the specification of a finite "slab".
This slab ensures at least some regularization of large coefficients and as a consequence,
more stable results.
This slab is governed by a degrees of freedom parameter ($\nu_3$, set to 4) and a scale parameter ($s$, set to 1).
This extension ensures greater numerical stability of the results.

The default settings discussed above are reasonable in most applications.
However, it is good practice to perform sensitivity analysis to determine how sensitive the model results are to different prior specifications. 
This is particularly important when the sample is small, as the prior is more influential in this case.

<span id = "tauprior">
The choice of prior distributions is an important decision in any Bayesian analysis.
This also applies to the heterogeneity parameters.
In the case of random effects meta-regression,
the only heterogeneity parameter is the between-studies variance, $\tau^2$.
In the case of three-level multilevel meta-regression,
there is a within-study and between-studies variance.

A crucial challenge with heterogeneity parameters in meta-regression is that the number of observations at the within- and between-study level is often small.
This can result in poor model convergence [@roverWeaklyInformativePrior2021],
or boundary estimates at zero [@chungAvoidingZeroBetweenstudy2013].
A well-known advantage of Bayesian meta-analysis is that it can overcome these challenges by using weakly informative priors, which guide the estimator towards plausible values for the heterogeneity parameters.
There is less consensus, however, about which priors are best for this purpose [@roverWeaklyInformativePrior2021].
By default, `brma()` uses a prior that was specifically developed for multilevel heterogeneity parameters
[@gelmanPriorDistributionsVariance2006]:
the half-Student's t distribution with large variance, $df = 3, scale = 2.5$.
Note that other suitable weakly informative priors have been discussed in the literature, but have not (yet) been implemented in `brma()`, such as the Wishart prior [@chungWeaklyInformativePrior2015], but has not been implemented in `brma()`.
There has also been increasing interest in the use of informative priors for heterogeneity parameters when information about their values is available [@thompsonGroupspecificPriorDistribution2020].
The use of informative priors is out of scope for BRMA, however, as BRMA takes a pragmatic approach to Bayesian analysis, using weakly informative priors to aid convergence for heterogeneity parameters,
and regularizing priors to perform variable selection for regression coefficients.
If researchers do wish to construct alternative prior specifications, they may want to develop a custom model in `rstan` instead [@rstan].
</span>

Unlike the frequentist LASSO algorithm, Bayesian regularized estimation does not shrink coefficients to be exactly equal to zero.
Therefore, variables must be selected post-estimation.
One way to do so is by the use of probability intervals, the Bayesian counterpart of confidence intervals, with a moderator being selected if, for example, a 95% interval excludes zero.
The present study considers two types of intervals:
The credible interval, which is obtained by taking the 2.5% and 97.5% quantiles of the posterior distribution, and the highest posterior density interval,
which is the narrowest possible interval that contains 95% of the probability mass.

<!--To solve this model, the moderator coefficients and the residual heterogeneity must be estimated simulataneously.
<!--The topic of estimating the residual heterogeneity is a highly discussed one (Veroniki et al., 2016; Viechtbauer \& López-López, 2015; Panityakul et al., 2013). The ability of the estimators to predict the residual heterogeneity is influenced by different factors, such as the number of studies (Guolo \& Varin, 2017; Panityakul et al., 2013; Hardy \& Thompson, 1996) included and the sample size of the individual studies (Panityakul et al., 2013).
A third, and obvious factor, that is classified as relevant to model performance is heterogeneity among studies being meta-analysed (Kontopantelis \& Reeves, 2011; Jackson \& White, 2018). Coverage from models degrades when the residual heterogeneity increases, mostly when the amount of studies is small (Brockwell \& Gorden, 2001). Considering that all models their performance is linked to the accuracy of the estimate. According to Sidik \& Jonkman (2007), it is generally the case that the larger true between-study variance is, the more biased the estimate can be, which diminishes the performance of the method.-->
<!--Numerous methods have been proposed to accurately estimate the residual heterogeneity, including the Hedges (HE), DerSimonian–Laird/Method of Moments (DL), Sidik and Jonkman (SJ), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), and Empirical Bayes (EB) method. These methods are mostly divided into two groups: closed-form or non-iterative methods and iterative methods. The main difference between these groups is that the closed form group uses a predetermined number of steps to provide an estimation for the residual heterogeneity, whereas the iterative methods run multiple iteration, as the name suggests, to converge to a solution when a specific criterion is met. It is important to note that some iterative methods do not produce a solution when they fail to converge after a predetermined amount of iteration. 

In our scenario we are especially interested in an estimator which performs well under the condition of a relative low number of studies. The Restricted Maximum Likelihood (REML) seems to produce the lowest bias under this condition and is therefore preferred (Panityakul et al., 2013; Hardy & Thompson, 1996). The REML is an iterative method and needs a starting estimation of $\tau^{2}$ to start, usually it gets estimated by one of the non-iterative methods (Viechtbauer \& López-López, 2015). Besides the starting value of $\tau^{2}$, it needs in every iteration an estimation of the regression coefficients of the moderators. These are typically estimated by using the Weighted Least Squares (WLS) method. This is a variation of the Ordinary Least Squares (OLS), but in the case of meta-analysis it is necessary to assess weights to the coefficients. In systematic reviews large variation in standard errors is often observed, which will result in large heteroscedasticity in the estimation of the effects (Stanley \& Doucouliagos, 2017). The addition of weights is a way to adjust for this heteroscedasticity. The weights are formulated as presented in equation (5). 

The usage of a WLS method to estimate the regression coefficient may be problematic in the situation where a lot of moderators are measured without their specific effects, when the amount of studies is low and when moderators are dichotomous. The use of a least squares method will cause problems with the prediction accuracy and the model interpretability (James, Witten, Hastie, \& Tibshirani, 2013). In the situation where a lot of moderators are measured and blindly included in the model, it may as well be the case that variables are included that are in fact not associated with the response. Including irrelevant variables in the model lowers the interpretability of the model (James et al., 2013). An approach is necessary that automatically excludes the variables that are irrelevant i.e. performs variable selection. As explained before, in meta-analysis it is often the case that the number of moderators closely approaches or even exceeds the number of studies included in the analysis. A least squares method will display a lot variability in the fit when the number of variables is not much smaller than the number of studies (James et al., 2013). This means that the least squares method over fits the data and loses its power to be generalizable to future observations. When the number of variables exceeds the number of studies, the least squares method fails to produce one unique estimate and the method should not be used at all. 

However, a least squares method could still be somewhat valuable in some situations. It is extremely suitable to estimate a linear relationship. In the case of dichotomous moderators, the relationship is always perfectly linear. A powerful non-linear estimation tool is in the situation of dichotomous moderators unnecessary and would not perform better at all. Whenever a non-linear relation gets fitted on data with an underlying linear relation, it will cause problems when this fit gets used for the prediction of future data. Given the various arguments, this paper provides an approach to tackle this problem of the least squares methods whilst still making use of a linear method. The weighted least squares are replaced with the so-called LASSO regression for the estimation of the regression coefficients. This algorithm shrinks or penalizes the regression coefficients and performs variable selection (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). -->

<!--
**Intro rma**
The rma algorithm is part of the software-package `metafor` in `R`, which is developed by Wolfgang Viechtbauer (2010, 2019). This algorithm is specifically developed to perform a meta-analysis or meta-regression. It allows to include different models, such as the fixed-, random- and mixed-effect model. It is also possible to account for moderators (Viechtbauer, 2010). The mixed-effect model, which is used is this study, requires a two-step approach to fit a meta-analytic model. First the residual heterogeneity is estimated. The package developed by Viechtbauer does provide multiple methods for the estimation of the residual heterogeneity. In this study the Restricted Maximum-likelihood is used, but this has already been discussed earlier. The second step is estimating the moderator coefficients, which is done by using the Weighted Least Squares (WLS) method. The weights are described in equation (5). The lma is a variation of the rma algorithm which is created by Caspar van Lissa. As explained before, the REML is an iterative procedure for the estimation of the residual heterogeneity. In every step of the process, instead of estimating the coefficients of the moderators by using a WLS, a weighted lasso regression is performed. Then again, the residual heterogeneity gets estimated with the rma algorithm by using the new values of the coefficients. With these new values of $\tau^{2}$, a new weighted lasso is performed for the estimations of the coefficients. This process continuous, until the residual heterogeneity converges to a certain value. -->

### Standardizing predictors

<!-- Eli? -->
<!-- * Eli, kan jij het algemene principe van standardization hier uitleggen, met een paar referenties?
In de volgende paragraaf over intercepts moet ook nog een referentie, over wel/niet standaardizeren van dummies.-->
<!-- * In BRMA werk het als volgt: -->
<!--     + By default, all variables are normalized. -->
<!--     + The penalized model (without an intercept) is estimated on the normalized data -->
<!--     + The intercept is then restored to the original scale by applying  -->
<!--     $b_0 = b_{0z} - \mathbf{b}_z\frac{\bar{\mathbf{x}}}{\mathbf{s}_x}$ -->
<!--     Where b_0 is the intercept, b_0z is the intercept for the normalized predictors, b_z is the vector of     regression coefficients for the normalized predictors, bar x is the vector of means of X, and s_x is the vector of standard deviations of x -->
<!--     + The regression coefficients are restored to the original scale by applying  -->
<!--     $\mathbf{b} = \mathbf{b}_z\mathbf{s}_x$ -->
<!-- * Leg vervolgens uit dat mensen dummies misschien niet willen standaardizeren -->
<!-- * Gebruikers kunnen de default standaardization overschrijven door een aantal van de variabelen handmatig te standaardizeren, en de vector met means en vector met SDs handmatig mee te geven via het argument `standardize = list(center = meanvec, scale = sdvec)`. Als ze een bepaalde variabele NIET willen standaardizeren moeten ze voor die variabele mean= 0 en sd= 1 meegeven. -->

Penalized regression analyses typically require the scales of predictors to be equivalent [@tibshirani1996].
This is because regularization penalizes coefficients equally, without regard for their scale.
If variables are on different scales,
this can lead to uneven penalization of coefficients in which
variables with smaller standard deviations are biased more strongly towards zero [@lee2015note].
To clarify, a regression parameter $\beta$ can be interpreted as the expected increase in outcome $y$ for a one unit increase in predictor $x$.
If the scale of predictor $x$ is increased by a factor 10, its regression coefficient is reduced by a factor 10.
Standardization is a widely used method for equalizing predictor scales,
in which the mean of all predictors is set to 0 and their standard deviation is set to 1 [@gelman2008scaling].
This type of standardization is also used by default in BRMA.

After standardization, the estimated parameters can be restored to their original scales.
For the intercept, the transformation is:
$$
b_0 = b_{0Z} - \mathbf{b}_Z\frac{\bar{\mathbf{x}}}{\mathbf{s}_X}
$$
where $b_0$ is the intercept, $b_{0Z}$ is the intercept for the standardized predictors,
$\mathbf{\bar{x}}$ and $\mathbf{s}_{x}$ are the vectors of predictor means and variances,
and $\mathbf{b_Z}$ is the vector of regression coefficients for the standardized predictors.
The regression coefficients are returned to their original scale by applying:
$$
\mathbf{b}_x = \frac{\mathbf{b}_z}{\mathbf{s}_x}
$$
Note that standardization is not always necessary or desirable.
If predictors are already on equivalent scales,
standardization does not make scales more equal, nor the penalization more fair
<!--Alternatively, if the goal of a study is to compare results to prior findings based on unstandardized predictors, standardization would make penalization unequal across studies.-->

There are additional considerations regarding standardization of categorical predictors [@alkharusi2012categorical].
As binary predictors can be straightforwardly included as predictors in linear models,
the most common way to represent categorical predictors is by choosing one response option as reference category,
and creating binary dummy variables to represent other response categories.
If these dummies are not standardized,
they might be unevenly penalized, as explained before.
However, standardizing dummy variables compromises the interpretability of their regression coefficients [@tibshiraniLassoMethodVariable1997; @wissmann2007role].
To illustrate this challenge, consider bivariate regression with a single binary predictor $x$ that takes on values 0 and 1 predicting outcome $y$.
The intercept represents the expected value of $y$ when $x$ is equal to zero, and the regression coefficient represents the difference in the expected value of $y$ between the two conditions [@alkharusi2012categorical].
By standardizing this binary predictor, the reference value is no longer zero, and both the intercept and its regression coefficient have no clear interpretation anymore.
Extending this example to the multivariate case further complicates the problem [@wissmann2007role]. 

The appropriate solution depends on the research goals;
if the primary goal is variable selection,
then the dummies should be standardized.
However, if the primary goal is interpretation of the coefficients,
they should not be [@gelman2008scaling].
A related challenge is that, whereas various coding schemes for categorical predictors are equivalent in OLS regression, this is not the case in penalized regression.
The choice of coding affects model fit and interpretation of the coefficients [@chiquet2016coding; @detmer2020note].

### Intercepts

The standard linear model estimates an intercept, which reflects the expected value of the outcome when all predictors are equal to zero, and regression coefficients for the effect of moderators.
In some cases, it may be desirable to omit the intercept.
For example, if an analysis contains categorical predictors,
these can be encoded as dummy variables, with values $x \in \{0, 1\}$.
For a variable with $c$ categories, the number of dummy variables must be equal to $c-1$;
the omitted category functions as a reference category, and its expected value is represented by the model intercept $b_0$.
This so-called *regression specification* of a model may be useful when there is a meaningful reference category.
For example, imagine a study on the effectiveness of interventions for specific phobia with two interventions: Treatment as usual, and a novel intervention.
In this case, it might make sense to code treatment as usual as the reference category,
and dummy-code the new contender.
The model will then estimate whether the newly developed intervention has an effect size significantly lower or higher than the industry standard.
In other cases, there may not be a straightforward reference category.
For example, imagine a study on the effectiveness of one intervention for specific phobia in two continents.
In such cases, the average effect in both continents may be estimated by omitting the intercept, and including all $c$ dummy variables.
This so-called *ANOVA specification* of a model estimates a mean for all dummy-coded categories.

<!-- This part seems a repetition from the info on standardization of dummies above?: 
The appropriate standardization of dummy variables in penalized regression analyses is a topic of ongoing debate.
Suffice it to say that, for greater interpretability, users may want the freedom to leave dummy variables unstandardized.
This is possible in `brma()` by ... 
-->

## Implementation

We implemented BRMA in the function `brma()` in the statistical programming language R [@R-base],
in the package `pema`, short for *pe*nalized *m*eta *a*nalysis.
The `brma()` function aims to make Bayesian regularized regression readily available via a user-friendly interface.
R-users can install the package from CRAN, by running `install.packages("pema")`.
Non-R-users can use BRMA via the "Penalized Meta-Analysis" extension of JASP [@JASP2022],
a free open source statistical software package with a graphical user interface, see Figure \@ref(fig:figjasp).

```{r figjasp, fig.cap="Using BRMA via the JASP software package." }
knitr::include_graphics("jasp.png")
```

For estimation, `brma()` depends on Stan, a probabilistic programming language that uses Hamiltonian Monte Carlo to sample from the posterior distribution [@StanManual].
Stan is written in C++, and thus computationally efficient, but custom models must be compiled prior to estimation.
This results in substantial computational overhead,
and installing a toolchain to compile models requires some technical sophistication.
To avoid this overhead, `pema` uses pre-compiled stock models with opinionated default options.
At the time of writing, these include random effects and three-level meta-regression with and without an intercept.
R-users can refer to the package documentation to see what options are available at the time of reading by running `?pema::brma`.
Researchers who wish to construct a model that is currently out of scope of `brma()` are referred to `rstan` instead [@rstan].
As a starting point, the `rstan` source code for the stock models included with `pema` is available in `pema:::stanmodels`.
We welcome contributions of additional models.

The function `brma()` has two main interfaces:
a formula interface, corresponding to base-R functions like `lm()`,
in which the user provides a model `formula` that references variables in a `data` argument.
The second interface is more amenable to machine learning applications,
and accepts an `x` matrix of predictors and a `y` vector of effect sizes.
Additionally, `brma()` has an argument `vi`, which refers to the effect size variances,
and `study`, which (optionally) refers to a clustering variable for three-level meta-regression.
Both of these arguments accept either the name of a column in `data`, or a numeric vector.

As mentioned before, the R-implementation of BRMA has several options that can be customized.
The most important option relates to the choice of priors f the regression coefficients.
At the time of writing, `brma()` supports two priors for regression coefficients: the LASSO and the regularized horseshoe.
A prior is selected using the `method` argument;
the `prior` argument is used to specify custom values for the prior hyperparameters (see Statistical underpinnings).
The lasso prior uses Laplace priors, whose scale is determined by the `scale` parameter multiplied with a scale parameter,
which in turn is assigned a chi-square prior distribution with `df` degrees of freedom.
Increasing `df` allows for larger values for the inverse-tuning parameter, leading to less shrinkage.

The horseshoe prior has several shrinkage parameters,
all assigned Student's t prior distributions with parameters `df` and `scale`.
The `df` parameter controls the thickness of the tails,
with higher values corresponding to thinner tails, which assign less probability mass to extreme values.
The `scale` parameter controls how wide the prior is;
smaller values assign most probability mass to values near zero, thus resulting in more regularization.
The parameters `df` and `scale` are local shrinkage parameters, enabling flexible shrinkage of separate regression coefficients.
Parameters `df_global` and `scale_global` control global shrinkage that influences all coefficients similarly.
The regularized horseshoe applies additional regularization to very large coefficients,
which is governed by parameters `df_slab` and `scale_slab`.
This additional regularization ensures at least some shrinkage of large coefficients to avoid any sampling problems.
When prior information regarding the expected number of relevant moderators is available,
this information can be incorporated via the `relevant_pars` argument.
The `scale_global` argument is then ignored and instead calculated based on `relevant_pars`.

Another important decision is whether or not to standardize parameters.
By default, `brma()` standardizes the predictor matrix,
and restores model coefficients to their original scale,
as explained in Statistical underpinnings.
There are two ways to circumvent this default standardization.
The first is to disable standardization entirely, analyzing predictors in their original scale,
by setting `standardize = FALSE`.
Alternatively, `brma()` allows custom standardization.
To use this option, first manually standardize (some of) the predictors.
Then, when calling `brma()`, provide the means (`means`) and standard deviations (`sds`) that should be used to restore coefficients to the predictors' original scale.
This can be accomplished using the argument `standardize = list(center = means, scale = sds)`.
This approach can also be used to select predictors that **should not** be standardized:
For these predictors,
simply pass a mean of 0 and a standard deviation of 1;
this leaves the coefficient in question unaffected.

# Simulation study

The present simulation study set out to validate the BRMA algorithm.
As a benchmark for comparison, we used restricted maximum likelihood meta-regression,
which is the standard in the field.
We evaluated the algorithms' predictive performance in new data, 
<!-- examining its predictive performance in new data, its sensitivity and specificity in variable selection, and its bias and variance in recovering population parameters. -->
<!-- their ability to perform variable selection, -->
and their ability to recover population parameters. 
Our research questions are whether BRMA offers a performance advantage over state-of-the-art random effects meta-analysis using restricted maximum likelihood [RMA, @viechtbauerConductingMetaanalysesMetafor2010] in terms of any of these indicators,
and which prior (regularized horseshoe versus LASSO) is to be preferred.
All analysis code is available in a version-controlled repository at <https://github.com/cjvanlissa/pema>.

## Performance indicators

Predictive performance reflects how well the algorithm is able to predict data not used to estimate the model parameters, in other words, it indicates the generalizability of the model.
To compute it, for each iteration of the simulation both a training dataset and a testing dataset are generated.
The model is estimated on the training data, which has a varying number of cases according to the simulation conditions.
Predictive performance is then operationalized as the explained variance in the testing data, $R^2_{test}$.
The testing data has 100 cases in all simulation conditions.
The $R^2_{test}$ reflects the fraction of variance in the testing data explained by the model,
relative to the mean.
Note that the mean of the training data, not of the testing data, is used as a benchmark.
The resulting metric $R_{test}^{2}$ is expressed by the following equation:

$$
R_{test}^{2} = 1- \frac{\sum_{i=1}^{k}(y_{i-test}-\hat{y}_{i-test})^{2}}{\sum_{i=1}^{k}(y_{i-test}-\bar{y}_{train})^{2}}
$$

With $k$ being the number of studies in the testing dataset, $\hat{y}_{i-test}$ being the predicted effect size for study $i$, and $\bar{y}_{train}$ being the mean of the training dataset.

The algorithms' ability to perform variable selection was evaluated by sensitivity and specificity.
Sensitivity $P$ is the ability to select true positives, or the probability that a variable is selected, $S = 1$, given that it has a non-zero population effect: $P = p(S = 1||\beta| >0)$.
Specificity is the ability to identify true negatives, or the probability that a variable is not selected given that it has a zero population effect: $N = p(S = 0|\beta  = 0)$.

The ability to recover population parameters $\beta$ and $\tau^2$ was examined in terms of bias and variance of these estimates.
The bias is given by the mean deviation of the estimate from the population value,
and the variance is given by the variance of this deviation.

## Design factors

<span id = "designfactors">To examine performance in a range of realistic meta-analysis scenarios,
seven design factors were manipulated:
First, we manipulated the number of studies in the training data $k \in (20, 40, 100)$. 
Second, the average within-study sample size $\bar{n} \in (40, 80, 160)$. 
Third, 
true effect sizes were simulated according to two models:
one with a linear effect of one moderator, $T_{i}= \beta x_{1i} + \epsilon_i$, and one with a non-linear (cubic) effect of one moderator, $T_{i}= \beta x_{1i} + \beta x_{1i}^{2} + \beta x_{1i}^{3} + \epsilon_i$,
where $\epsilon_i \sim N(0, \tau^2)$.
As both BRMA and RMA assume linear effects,
simulating data from a non-linear model allows us to examine how robust the different methods are to violations of this assumption.
The fourth design factor was the population effect size $\beta$ in the aforementioned models, with $\beta \in (0, .2, .5, .8)$. 
Fifth, we manipulated the residual heterogeneity $\tau^2$ in the aforementioned models, with $\tau^{2} \in (.01, .04, .1)$.
According to a review of 705 published psychological meta-analyses (Van Erp et al., 2017),
these values of $\tau^2$ fall within the range observed in practice.
Sixth, we varied the number of moderators not associated with the effect size $M \in (1, 2, 5)$.
These are the moderators that ought to be shrunk to zero by BRMA.
Note that the total number of moderators is $M+1$,
as one moderator is used to compute the true effect size (see the third design factor).
Finally, moderator variables were simulated as skewed normal moderators, with scale parameter $\omega \in (0, 2, 10)$, where $\omega = 0$ corresponds to the standard normal distribution.</span>
All unique combinations of these design factors produced `r out$conditions` unique conditions.
For each simulation condition, 100 data sets were generated.
In each data set, the observed effect size $y_i$ was simulated as a standardized mean difference (SMD),
sampled from a non-central $t$-distribution.

<!-- **het aantal moderatoren is redelijk laag; ik zou vooral nog voordelen van brma verwachten met meer moderatoren maar is dat realistisch in de praktijk? Zo ja, dan is het iets om te noemen in de discussie** hier ook nog een eerdere comment. Mocht je een keer een toepassing tegenkomen met echt veel moderatoren, kan het interessant zijn hier nog een kleine simulatie op te baseren. Een voorbeeld kan zijn een set van moderatoren die de kwaliteit van studies proberen te vatten zoals we laatst hebben besproken -->


<!-- x = rnorm(1000) -->
<!-- func <- function(x, b){b*x+(b*(x^2))+(b*(x^3))} -->
<!-- dat <- data.frame(x = x, -->
<!--                   y = c(func(x, .2), func(x, .5), func(x, .8)), -->
<!--                   es = rep(c(".2", ".5", ".8"), each = 1000)) -->
<!-- ggplot(dat, aes(x, y, color = es))+geom_smooth() -->

# Results

Any iterative algorithm is susceptible to convergence problems.
In such cases, the BRMA algorithms provide warning messages,
but still return samples from the posterior.
We were thus able to use all iterations of the BRMA algorithms,
although there may be some that failed to converge, which will likely have poor performance.
When the RMA algorithm fails to converge, however, it terminates with an error.
To handle this contingency, we automated some of the steps recommended [on the `metafor` website](https://www.metafor-project.org/doku.php/tips:convergence_problems_rma).
Nevertheless, `r out$missing` replications of the RMA algorithm failed to converge.
All of these were characterized by low number of cases ($k \leq 40$) and high effect sizes $\beta \geq .5$.
These cases were omitted from further analysis.

## Predictive performance
```{r}
out <- readRDS("output.RData")
whichhi <- prop.table(out$which_highest)
```

Within data sets, the BRMA with a horseshoe prior had the highest predictive performance `r round(whichhi[1]*100)`% of the time, followed by RMA, `r round(whichhi[3]*100)`%, and finally BRMA with a LASSO prior, `r round(whichhi[2]*100)`%.
Results indicated that the overall $R^2_{test}$ was highest for BRMA with a horseshoe prior and lowest for RMA, see Table \@ref(tab:tabr2).
This difference was driven in part by the fact that explained variance was somewhat higher for the BRMA models when the true effect was non-zero (i.e., in the presence of a population effect),
and by the fact that RMA had larger negative explained variance when the true effect was equal to zero (i.e., there was no population effect to detect).

```{r tabr2, results= "asis"}
tab <- read.csv("r2.csv")[1:7][c(1, 2,5,3,6,4,7)]
#[, c(seq(1, 9, by = 3), seq(2, 9, by = 3),
#                                                                  seq(3, 9, by = 3))]
names(tab)[1] <- ""
names(tab) <- gsub(".", " ", names(tab), fixed = T)
names(tab)[grepl("Mean", names(tab))] <- paste0(names(tab)[grepl("Mean", names(tab))], "}$")
names(tab) <- gsub("Mean R2", "$\\bar{R^2}_{", names(tab), fixed = T)
names(tab) <- gsub("SD R2.*", "$SD$", names(tab))
names(tab) <- gsub("CI.*", "$CI_{95}$", names(tab))
kbl(tab[, ], caption = "Mean and SD of predictive R2 for BRMA with a horseshoe (HS) and LASSO prior, and for RMA, for models with a true effect (ES != 0) and without (ES = 0).", digits = 2, row.names = FALSE, escape = FALSE, booktabs = TRUE, col.names = gsub("\\.\\d", "", names(tab)))
  
```

The effect of the design factors on $R^2_{test}$ was evaluated using ANOVAs.
Note that p-values are likely not informative due to the large sample size and violation of the assumptions of normality and homoscedasticity.
The results should therefore be interpreted as descriptive, not inferential, statistics.
Table \@ref(tab:tabr2eta) reports the effect size $\eta^{2}$ of simulation conditions on $R^2_{test}$.
<!-- for main effects and two-way interactions between simulation conditions. -->
<!-- , -->
<!-- we provide an overview of the interpretation of those effects in the Table. -->
<!-- For interaction effects, we report whether the interaction was spreading -->
<!-- Interpretation was less straightforward for a number of interactions; -->
<!-- for these, the median $R^2_{test}$ is graphed in Figure \@ref(fig:figanova). -->

```{r tabr2eta, results= "asis", eval = TRUE}
tabanova <- readRDS("anova.RData")
tabanova <- tabanova[!grepl(":", tabanova$condition), ]
tabanova$Interpretation[!tabanova$Interpretation %in% c("positive", "negative", "other")] <- NA
#tabanova$condition <- gsub("alpha", "omega", tabanova$condition, fixed = T)
#tabanova$condition <- gsub("$mean{n}$", "$n$", tabanova$condition, fixed = T)
tabanova$Factor <- gsub("\\$(omega|beta|tau)", "\\$\\\\\\1", tabanova$Factor)
#tabanova$HS.vs..LASSO <- NULL
tabanova <- tabanova[, c(ncol(tabanova), 2:(ncol(tabanova)-1))]
# kbl(tabanova, caption = "Effect size of design factors and their two-way interactions on R2 the different algorithms, and of the difference between algorithms. The comparison between HS and LASSO was zero in the second decimal for all conditions. Interpretation indicates whether the effect was uniformly positive or negative (for interactions, the effect of the second design factor in the interaction) for all algorithms.", digits = 2, row.names = FALSE, escape = FALSE, booktabs = TRUE, col.names = gsub("(?<!vs)\\.", " ", names(tabanova), perl = TRUE)) |>
#   kable_styling(latex_options = c("scale_down"))
#tabanova <- tabanova[!grepl(":", tabanova$condition, fixed = T), ]
apa_table(tabanova, caption = "Effect size of design factors on predictive R2 of the different algorithms, and of the difference between algorithms. Interpretation indicates whether a main effect was uniformly positive or negative across all algorithms.", digits = 2, escape = FALSE, col.names = gsub("(?<!vs)\\.", " ", names(tabanova), perl = TRUE))#, longtable = TRUE)
# Interpretation for main effects indicates whether the effect was uniformly positive or negative or other, and for interactions, whether it is a spreading or crossover interaction.
```
```{r figr2, fig.cap="Predictive R2 for BRMA with horseshoe (HS) and LASSO prior, and RMA. Plots are sorted by largest performance difference between BRMA and RMA."}
knitr::include_graphics("r2.png")
```

```{r figanova, fig.cap="Predictive R2 for HS (circle, solid line), LASSO(triangle, dotted line) and RMA (square, dashed line) for interactions. One design factor is displayed in different panels, the other on the x-axis.", eval = FALSE}
knitr::include_graphics("other_effects.png")
```

To test our research questions, we computed interactions of algorithm (HS vs. LASSO, HS vs. RMA and LASSO vs. RMA) with the other design factors.
The $\eta^2$ of these differences between algorithms are also displayed in Table \@ref(tab:tabr2eta).
Note that $\eta^2$ for the comparison between HS and LASSO was zero in the second decimal for all conditions; thus, this comparison was omitted from the Table.
The effect of design factors by algorithm is displayed in Figure \@ref(fig:figr2); these plots have been ranked from largest difference between BRMA and RMA to smallest.
Results indicate that the largest differences between algorithms were due to the effect size $\beta$, number of irrelevant moderators $M$, and the number of cases in the training data $k$.
Evidently, predictive performance increased most for the HS algorithm when the effect size increased above zero.
As noted previously, predictive performance of RMA was most negative (negative explained variance) when the effect size was zero.
The HS algorithm furthermore had the consistently highest predictive performance regardless of number of irrelevant moderators or number of cases in the training data, and was relatively less affected by increases in the number of irrelevant moderators (panel b) or in the number of training cases (panel c).
<!-- Similarly, the HS algorithm furthermore had the consistently highest predictive performance regardless of number of cases in the training data, and thus also increased less when the number of training cases increased (panel c). -->
Conversely, RMA had relatively poor predictive performance on average, and was more responsive to increases in the number of training cases and irrelevant moderators.
<!--However, differences between the algorithms were generally small. CJ: Niet mee eens; de verschillen zijn misschien klein in termen van de substantieve interpretatie van R2, maar de grootte van dat verschil hangt af van de gebruikte simulatie condities. Je kan dus een groter verschil aantonen als je daarmee gaat spelen, en een situatie opzoekt die ongunstiger is voor RMA. Belangrijker is dat we uberhaupt een consequent voordeel van BRMA over RMA aantonen-->

```{r figmaindif, fig.cap="Predictive R2 for design factors with largest differences in predictive performance across the HS (circle, solid line), LASSO(triangle, dotted line) and RMA (square, dashed line) algorithms.", eval = FALSE}
knitr::include_graphics("main_diff.png")
```

<!-- Results show that the true effect size $\beta$ had the largest marginal effect on $R^{2}$ for all algorithms. -->
<!-- As $\beta$ increased, each algorithm's performance increased as well. -->
<!-- Predictably, the effect of $\beta$ interacted with the model used to simulate data. -->
<!-- Compared to the linear model, $R^{2}_{test}$ for the cubic model showed diminishing increases with $\beta$. -->
<!-- The third largest marginal effect was that of the simulation model. -->
<!-- All algorithms had higher $R^2_{test}$ under the cubic model than under the linear model, which makes sense because $\beta$ had an additional effect through the polynomials of the relevant moderator. -->
<!-- There was an interaction between the model and the amount of skewness of the predictor variable $\omega$: -->
<!-- For all algorithms, the association between skewness and $R^2_{test}$ was negative for the linear model, and positive for the cubic model. -->
<!-- <!-- Figure \@ref(fig:skewness) shows the relationship. -->
<!-- The residual heterogeneity $\tau^{2}$ had a negative linear relationship with $R^{2}_{test}$ for all algorithms. -->
<!-- The mean sample size per study $\bar{n}$ also had a moderate effect for all algorithms, indicating that a greater number of participants within studies was positively linearly related with $R^{2}_{test}$. -->
<!-- Similarly, the number of studies in the training data $k$ was positively related with $R^{2}_{test}$. -->

<!-- Finally, the number of moderators did not have a big effect for the Pema algorithms, while for RMA and MetaForest the effect was more noticeable. The relationship is shown in image 7. The relationship is generally negative with more moderators meaning worse performance, although an increase can be observed as the number of moderators increase from 4 to 6 for all algorithms except MetaForest. This increase in performance for MetaForest appears when the number of moderators go from 3 to 4. -->


## Variable selection
```{r}
selprob <- out$selection
colnames(selprob) <- c("HS", "LASSO", "RMA")
truepos <- paste0(sapply(order(selprob[1,], decreasing = TRUE), function(i)paste0("$P_{", colnames(selprob)[i], "}", report(selprob[1,i]), "$")), collapse = ", ")
trueneg <- paste0(sapply(order(selprob[2,], decreasing = TRUE), function(i)paste0("$N_{", colnames(selprob)[i], "}", report(selprob[2,i]), "$")), collapse = ", ")
proboveral <- colMeans(selprob)
proboveral <- paste0(sapply(order(proboveral, decreasing = TRUE), function(i)paste0("$Acc_{", colnames(selprob)[i], "}", report(proboveral[i]), "$")), collapse = ", ")
```

To determine the extent to which the algorithms could perform variable selection correctly, the sensitivity to true positives $P$ and specificity to true negatives $N$ were calculated.
Only simulation conditions with $\beta > 0$ were used, such that the effect of the first moderator was always positive in the population and could be used to calculate $P$,
and the effect of the second moderator was always zero in the population and could be used to calculate $N$.
Additionally, overall accuracy can be computed, which reflects the trade off between sensitivity and specificity.
As the base rate of true positives and true negatives is equal in this simulation, overall accuracy is simply given by $Acc = (P+N)/2$.

As the regularized algorithms shrink all coefficients towards zero, it is unsurprising that sensitivity was highest for the un-regularized algorithm RMA, followed by HS and LASSO, `r truepos`.
By contrast, specificity was higher for the regularized algorithms, `r trueneg`.
Overall accuracy was approximately equal for RMA and HS, and was lower for LASSO, `r proboveral`.

Cramer's V, an effect size for categorical variables, was used to examine the effect of design factors on sensitivity (Table \@ref(tab:tabcramerp), Figure \@ref(fig:figsensitivity)) and specificity (Table \@ref(tab:tabcramern), Figure \@ref(fig:figspecificity)).
We also computed this effect size for the difference between algorithms in the number of true positives by design factor.
```{r tabcramerp, results= "asis", eval = TRUE}
tabselect <- readRDS("selected.RData")
rownames(tabselect) <- NULL
names(tabselect) <- gsub("(?<!vs)\\.", " ", names(tabselect), perl = TRUE)
apa_table(tabselect, caption = "Effect size (Cramer's V) of design factors, and of the difference between algorithms, on sensitivity (P).", digits = 2, escape = FALSE)#, longtable = TRUE)
# Interpretation for main effects indicates whether the effect was uniformly positive or negative or other, and for interactions, whether it is a spreading or crossover interaction.
```
```{r figsensitivity, fig.cap="Sensitivity by design factors for the HS (circle, solid line), LASSO(triangle, dotted line) and RMA (square, dashed line) algorithms."}
knitr::include_graphics("sensitivity.png")
```
```{r tabcramern, results= "asis", eval = TRUE}
tabselectn <- readRDS("notselected.RData")
rownames(tabselectn) <- NULL
names(tabselectn) = gsub("(?<!vs)\\.", " ", names(tabselectn), perl = TRUE)
apa_table(tabselectn, caption = "Effect size (Cramer's V) of design factors, and of the difference between algorithms, on specificity (N).", digits = 2, escape = FALSE)#, longtable = TRUE)
```
```{r figspecificity, fig.cap="Specificity by design factors for the HS (circle, solid line), LASSO(triangle, dotted line) and RMA (square, dashed line) algorithms."}
knitr::include_graphics("specificity.png")
```
Differences in sensitivity between the algorithms were near-zero for HS and LASSO.
The difference between the two BRMA algorithms and RMA were largest for the design factor effect size $\beta$, followed by the model and number of studies $k$.
Across all design factors, RMA had the highest sensitivity, followed by HS and then LASSO.

For specificity, differences in sensitivity between HS and LASSO were largest for the number of noise moderators $M$, followed by the effect size $\beta$, number of studies $k$, and residual heterogeneity $\tau^2$.
The difference between the two BRMA algorithms and RMA were largest for the design factor number of studies $k$, followed by the model, the number of noise moderators $M$, and the effect size $\beta$.
Across all design factors, HS had the highest specificity, followed by LASSO and then RMA.
Also note that the association between design factors and specificity was not monotonously positive or negative across algorithms.
Instead, some design factors had opposite effects for the two BRMA algorithms versus RMA.
For instance, a larger number of studies $k$ had a negative effect on specificity for the BRMA algorithms, but a positive effect for RMA - within the context that RMA had lower specificity on average.
Conversely, a greater number of noise moderators $M$ had a positive effect on specificity for BRMA, but a negative effect for RMA.

## Ability to recover population parameters

The ability to recover population parameters $\beta$ and $\tau^2$ was examined in terms of bias and variance of these estimates.
If the value of the regression coefficient as estimated by one of the algorithms is $\hat{b}$,
then the bias $B$ and variance $V$ of this estimate can be computed as the mean and variance of the difference between $\hat{b}$ and $\beta$ across simulation conditions, respectively.
Across all simulation conditions, HS had the lowest bias for $\tau^2$, $B_{HS} `r report(out[["tau2_bias"]][["hs_tau2"]])`$, 
followed by RMA, $B_{RMA} `r report(out[["tau2_bias"]][["rma_tau2"]])`$,
and then LASSO, $B_{LASSO} `r report(out[["tau2_bias"]][["lasso_tau2"]])`$.
Note that all algorithms yielded positively biased estimates. 
The LASSO estimates of $\tau^2$ had the lowest variance, $V_{LASSO} `r report(out[["tau2_variance"]][["lasso_tau2"]])`$, 
followed by HS, $V_{HS} `r report(out[["tau2_variance"]][["hs_tau2"]])`$,
and then RMA, $B_{RMA} `r report(out[["tau2_variance"]][["rma_tau2"]])`$.
The effect of the design factors on the bias in $\tau^2$ was evaluated using ANOVAs.
Table \@ref(tab:tabtau2) reports the effect size $\eta^{2}$ of simulation conditions on $\hat{t^2}-\tau^2$.
The design factors $\beta$ and model had the largest effect on bias in estimated $\tau^{2}$ for all algorithms.
No differences between algorithms in the effect of design factors were observed.

```{r tabtau2, results= "asis", eval = TRUE}
tabtau <- readRDS("table_tau2.RData")
tabtau$Factor <- gsub("\\$(omega|beta|tau)", "\\$\\\\\\1", tabtau$Factor)
tabtau <- tabtau[, c(ncol(tabtau), 2:(ncol(tabtau)-1))]
apa_table(tabtau, caption = "Effect size of design factors on bias in tau squared for the different algorithms, and of the difference between algorithms.", digits = 2, escape = FALSE, col.names = gsub("(?<!vs)\\.", " ", names(tabtau), perl = TRUE))
```

For the estimated regression coefficient, HS had the greatest (negative) bias across simulation conditions, $B_{HS} `r report(out[["beta_bias"]][["hs_beta1"]])`$, 
followed by LASSO, $B_{LASSO} `r report(out[["beta_bias"]][["lasso_beta1"]])`$,
and then RMA, $B_{RMA} `r report(out[["beta_bias"]][["rma_beta1"]])`$.
Note that all algorithms - including RMA - provided, on average, negatively biased estimates. 
Across simulation conditions, HS had the lowest variance, $V_{HS} `r report(out[["beta_variance"]][["hs_beta1"]])`$, followed by LASSO, $B_{LASSO} `r report(out[["beta_variance"]][["lasso_beta1"]])`$,
and then RMA, $B_{RMA} `r report(out[["beta_variance"]][["rma_beta1"]])`$.
The effect of the design factors on the bias in estimated $\beta$ was evaluated using ANOVAs.
Table \@ref(tab:tabbeta) reports the effect size $\eta^{2}$ of simulation conditions on $\hat{b}-\beta$.
The skewness of moderator variables had the largest effect on bias in estimated $\beta$ for all algorithms.
Note, however, that this is likely due to the fact that the data simulated with a cubic model are analyzed with a linear model, and thus,  
was the estimated model.
This was mainly because the algorithms overestimated $\tau^{2}$ most when the model contained cubic terms.
No differences between algorithms in the effect of design factors were observed.

```{r tabbeta, results= "asis", eval = TRUE}
tabbeta <- readRDS("table_beta.RData")
tabbeta$Factor <- gsub("\\$(omega|beta|beta)", "\\$\\\\\\1", tabbeta$Factor)
tabbeta <- tabbeta[, c(ncol(tabbeta), 2:(ncol(tabbeta)-1))]
apa_table(tabbeta, caption = "Effect size of design factors on bias in beta squared for the different algorithms, and of the difference between algorithms.", digits = 2, escape = FALSE, col.names = gsub("(?<!vs)\\.", " ", names(tabbeta), perl = TRUE))
```

# Applied example

In this application, we will work with the `pema::bonapersona` data [@BonapersonaEtal2019].
This meta-analysis of over 400 experiments investigated the effects of early life adversity on cognitive performance in rodents.
This example uses a small subset of the more than 30 moderators.
See the `pema` package documentation (help and vignettes) for further examples.
<!-- We focus specifically on how to choose the different hyperparameters of the regularized horseshoe prior. -->

```{r exdata, eval = TRUE}
# Load relevant packages
library(pema)
# Select data to analyze
df <- bonapersona[ , c("yi", "vi", "mTimeLength", "year", "model", "ageWeek")] 
# Impute missings
df$ageWeek[is.na(df$ageWeek)] <- median(df$ageWeek, na.rm = TRUE)
```

Our simulation study shows good performance with default hyperparameters.
However, experienced users may want to customize the prior.
Visualizing the prior can be helpful in this process.
This is accomplished using the interactive application visualization application available through `shiny_prior()`.
The user can plot the prior distributions resulting from different sets of hyperparameters and compare them.
Increasing the values of the scale parameters (`scale_global` and `hs_scale_slab`) results in a more spread out prior, which applies less regularization.
Increasing the degrees of freedom (`df_global` and `df_slab`) results in thinner tails, which applies more regularization.
<!-- One advantage of the regularized horseshoe prior is its ability to define the prior in terms of the expected number of nonzero coefficients, using the argument `relevant_pars`. -->
<!-- This argument in turn influences the other hyperparameters. -->
<!-- We can also plot the implied prior on the effective number of nonzero coefficients given specific hyperparameters. -->
<!-- In this application, we have a total of 7 possible moderators, including the dummy variables for the categorical moderator "model". We can visualize the implied prior on the number of nonzero coefficients for different values of `relevant_pars`: p0 = 1; p0 = 3; p0 = 6; or p0 = NA, in which case no prior information is given and `scale_global`= 1. -->

```{r exprior, eval = FALSE, include = FALSE}
mod <- stan_model("../inst/stan/prior_meff.stan")
datdum <- data.frame(model.matrix(~., df)[, -1])
meff_prior_fun <- function(priorinfo = TRUE, p0){
  D <- ncol(datdum) - 2
  n <-  nrow(datdum)
  sigma <- 1
  
  vars <- apply(datdum, 2, var)[-c(1:2)]
  
  if(priorinfo == TRUE){
    tausq0 <-  p0/(D-p0)*(sigma/sqrt(n))
  } else if(priorinfo == FALSE){
    p0 <- NA
    tausq0 <- 1
  }
  
  dat <- list(tausq0 = tausq0,
              D = D,
              n = n,
              sigma = sigma, # fixed to 1 in the current implementation; alternative would be to scale to tau
              s2 = vars)
  fit <- sampling(mod, data = dat, iter = 4000)
  draws <- as.matrix(fit)
  df <- cbind.data.frame("p0" = paste0("p0 = ", p0),
                         "draws" = draws[, "meff"])
  return(df)
}

df1 <- meff_prior_fun(priorinfo = FALSE)
df2 <- meff_prior_fun(priorinfo = TRUE, p0 = 1)
df3 <- meff_prior_fun(priorinfo = TRUE, p0 = 3)
df4 <- meff_prior_fun(priorinfo = TRUE, p0 = 6)
df <- rbind.data.frame(df1, df2, df3, df4)

# plot
ggplot(df, aes(x=draws)) + 
  geom_histogram() +
  facet_wrap(~p0) +
 theme_bw() + 
  xlab("Effective number of nonzero coefficients") + 
  ylab("") +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```
<!-- It is clear that the implied prior on the number of nonzero coefficients follows the prior information it has been given, with more mass on a small number of relevant moderators if that is expected a priori (p0 = 1) and more mass on a larger number of relevant moderators if that is expected a priori (p0 = 6). Interestingly, when no prior information is provided (p0 = NA), the default `global_scale` setting of 1 implies a relatively large number of nonzero coefficients, which is important to keep in mind when using this default setting. -->

As no prior is specified, this example uses a horseshoe prior with default hyperparameters.
To see the default values, open the function documentation using `?brma`.

```{r exmodel1, echo = TRUE, eval = FALSE}
fit <- brma(yi ~ ., data = df, vi = "vi")
```
```{r, echo = FALSE, eval = run_everything}
fit <- brma(yi ~ ., data = df, vi = "vi")
saveRDS(fit, "fit.RData")
```
```{r, echo = FALSE, eval = TRUE}
fit <- readRDS("fit.RData")
```

By running `summary(fit)`, we obtain the posterior mean, standard deviation, and quantiles of the model parameters (see Table \@ref(tab:tabsumex)).
Use the posterior mean or median (50% quantile) and 95% credible interval (2.5% - 97.5%) to perform inference on model parameters.
<!-- In the simulation study, -->
<!-- we retained moderators when the 95% credible interval excluded zero, -->
<!-- Since Bayesian penalization does not automatically shrink estimates exactly to zero. -->
Parameters whose 95% credible interval excludes zero are marked with an asterisk.
Note that Bayesian analyses do not use the frequentist notion of significance.
Instead, we say that there is a 95% probability that the true population parameter lies within the interval,
given the prior and observed data.
In this example, however, there are no moderators for which the 95\% CI excludes zero.

```{r, echo = FALSE, eval = FALSE}
summary(fit)
```
```{r tabsumex, eval = TRUE, echo = FALSE, results = "asis"}
tab <- summary(fit)$coefficients[1:8, c("mean", "sd", "2.5%", "50%", "97.5%", "n_eff", "Rhat")]
papaja::apa_table(tab, caption = "Summary of model parameters for the applied example.")
```
Many additional convenience functions exist for `rstan` models,
which become available by converting a `brma` model object to a `stanfit` object,
using the function `as.stan(fit)`.
This makes it possible to plot the model parameters instead of tabulating them,
using the `plot()` function.
For example, one can obtain posterior density plots for parameters using `plot(as.stan(fit), plotfun = "dens", pars = c("Intercept", "year"))`.

```{r exres, eval = FALSE, echo = FALSE}
fit_stan <- as.stan(fit)
plot(fit_stan, pars = rownames(coef(fit))[2:8])
```
```{r, eval = run_everything, echo = FALSE}
fit_stan <- as.stan(fit)
p <- plot(fit_stan, pars = rownames(coef(fit))[2:8])
ggsave("plot_es.png", p, device = "png", width = 4, height = 3, dpi = 300, scale = 1.5)
```
```{r, eval = FALSE, echo = FALSE, fig.cap="Visual presentation of model parameters for the applied example."}
knitr::include_graphics("plot_es.png")
```

It is good practice to assess model convergence.
For example, the analysis above returns a warning about "divergent transitions".
Converting to a `stanfit` object also facilitates convergence diagnostics;
for example, using the function `check_hmc_diagnostics(as.stan(fit))`.
Additionally, the MCMC draws can be visualized using `traceplot(as.stan(fit), pars = c("Intercept", "year"))`.
The traces of a converged model look like "fat caterpillars",
with the different MCMC chains mixing together.
<!-- Note that there are many auxiliary parameters used for the regularized horseshoe prior. No information on these parameters is returned in the `brma` fitobject, however, once the fitobject is transformed into a Stan object for plotting, these parameters are included. -->

```{r extrace, eval = FALSE, echo = FALSE}
traceplot(fit_stan, pars = c("Intercept", "year"))
```
```{r, eval = FALSE, echo = FALSE}
p <- traceplot(fit_stan, pars = c("Intercept", "year"))
ggsave("traceplot.png", p, device = "png", width = 4, height = 2.5, dpi = 300, scale = 1.5)
```
```{r, eval = FALSE, echo = FALSE, fig.cap="Trace plot for the applied example."}
knitr::include_graphics("traceplot.png")
```

The model summary also offers convergence diagnostics.
For example, the column `Rhat` provides information on the split $\hat{R}$,
a version of the potential scale reduction factor [PSRF, @GelmanRubin1992].
Values close to 1 indicate convergence.
In addition, the column `n_eff` provides information on the number of effective (independent) MCMC samples, which should be high relative to the total number of samples (in this case, 4000).
In this example, all `Rhat` values are close to 1.
The effective number of MCMC samples is relatively small compared to the total number of MCMC samples.
An often used heuristic is to consider ratios smaller than 0.1 as problematic.
<!-- Even though this is not the case here, consider increasing the number of MCMC iterations. -->
<!-- These diagnostics can be easily visualized using the `bayesplot` package: -->
Both statistics indicate convergence in this example.

As mentioned before, this analysis results in a warning message about divergent transitions.
Divergent transitions can result in biased estimates.
However, the posterior distribution is often good enough to safely interpret the results if the number of divergences is small and there are no further indications of non-convergence.
In some cases, divergent transitions may be resolved by increasing the degrees of freedom of the prior.
Increasing both `df_global` and `df_slab` to 5 results in fewer divergences for this example,
but does not otherwise influence the substantive interpretation of the results.
It is prudent to perform similar sensitivity analyses to determine whether results are robust to different priors.

```{r exmodel2, eval = FALSE, echo = F}
fit2 <- brma(yi ~ .,
             data = df,
             vi = "vi",
             prior = c(df = 5, df_global = 5, df_slab = 4,
                       scale_global = 1, scale_slab = 1,
                       relevant_pars = NULL))
```
```{r, echo = FALSE, eval = FALSE}
fit2 <- brma(yi ~ .,
                data = df,
                vi = "vi",
                prior = c(df = 5, df_global = 5, df_slab = 4, scale_global = 1, scale_slab = 1, relevant_pars = NULL))
saveRDS(fit2, "fit2.RData")
```
```{r, echo = FALSE, eval = FALSE}
fit2 <- readRDS("fit2.RData")
```


```{r exrhat, eval = FALSE, echo = FALSE}
rhats <- rhat(fit)
mcmc_rhat(rhats[names(rhats) %in% parsel])
```

```{r exeffn, eval = FALSE, echo = FALSE}
ratios_neff <- neff_ratio(fit)
mcmc_neff(ratios_neff[names(ratios_neff) %in% parsel])
```

<!-- ### Results -->


<!-- ## Prior sensitivity -->

<!-- To assess the sensitivity of the results to the chosen values of the hyperparameters, -->
<!-- one can run the model with a different prior. -->
<!-- In this example, we vary the expected number of relevant moderators `relevant_pars`.  -->

```{r expriorsens, eval = FALSE, echo = FALSE}
fit3 <- brma(yi ~ .,
                data = datdum,
                vi = "vi",
                method = "hs",
                standardize = FALSE,
                prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 1, scale_slab = 1, relevant_pars = 1),
                mute_stan = FALSE)

fit4 <- brma(yi ~ .,
                data = datdum,
                vi = "vi",
                method = "hs",
                standardize = FALSE,
                prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 1, scale_slab = 1, relevant_pars = 3),
                mute_stan = FALSE)

fit5 <- brma(yi ~ .,
                data = datdum,
                vi = "vi",
                method = "hs",
                standardize = FALSE,
                prior = c(df = 1, df_global = 1, df_slab = 4, scale_global = 1, scale_slab = 1, relevant_pars = 6),
                mute_stan = FALSE)

# plot
df.fun <- function(fit, prior){
  plotdat <- data.frame(fit$coefficients)
  plotdat$par <- rownames(plotdat)
  plotdat$Prior <- prior
  return(plotdat)
}

df1 <- df.fun(fit, prior = "hs default")
df3 <- df.fun(fit3, prior = "hs with p0 = 1")
df4 <- df.fun(fit4, prior = "hs with p0 = 3")
df5 <- df.fun(fit5, prior = "hs with p0 = 6")

df <- rbind.data.frame(df1, df3, df4, df5)
df$par <- as.factor(df$par)
plotdat <- filter(df, !par %in% c("Intercept", "tau2"))

pd <- 0.5

ggplot(plotdat, aes(x=mean, y=par, group = Prior)) + 
  geom_errorbar(aes(xmin=X2.5., xmax=X97.5., colour = Prior), width=.1, position = position_dodge(width = pd)) +
  geom_point(aes(colour = Prior), position = position_dodge(width = pd)) +
  theme_bw() + xlab("Posterior mean") + ylab("")
```

<!-- When running the models with prior information regarding the number of relevant moderators, it becomes clear that there are convergence issues. The number of divergent transitions is very high and the maximum treedepth is often exceeded. For the model with `relevant_pars` = 1, these convergence issues are also reflected in high `rhat` values. -->
<!-- When comparing the results, it is clear that the horseshoe specifications that include prior information shrink coefficients more towards zero compared to the default setting. This is a consequence of the fact that these settings imply more prior mass on smaller numbers of relevant moderators, as was shown previously. However, in this situation, this high shrinkage results in convergence issues so these results should not be fully trusted but are only shown for the purpose of illustration. In our experience, the default prior settings are suitable for most applications, but conducting a prior sensitivity analysis as well as prior visualizations before running the analysis can help users to better understand the prior and its influence on the results. -->

# Discussion

This study presented a novel algorithm to select relevant moderators that can explain heterogeneity in meta-analyses, using Bayesian shrinkage priors. 
The simulation study validated the performance of two versions of the new BRMA algorithm, relative to state-of-the-art meta-regression (RMA).
Our analyses examined the algorithms' predictive performance, which is a measure of generalizability,
their ability to perform variable selection,
and their ability to recover population parameters.
Our research questions were whether BRMA offers a performance advantage over RMA in terms of any of these indicators,
and which prior (horseshoe versus LASSO) is to be preferred.

Results indicated that the BRMA algorithms had higher predictive performance than RMA in the presence of relevant moderators.
In the absence of relevant moderators, RMA produced overfit models; in other words, its models generalized poorly to new data.
The predictive performance of the BRMA algorithms also suffered less than that of RMA in the presence of more irrelevant moderators.
The BRMA algorithms were also more efficient, in the sense that they achieved greater predictive performance when the number of studies in the training data was low.
Across all conditions, BRMA with a horseshoe prior achieved the highest average predictive performance, and within each data set, BRMA with a horseshoe prior most often had the best predictive performance (in `r round(whichhi[1]*100)`% of replications).
Based on these findings, we would recommend using BRMA with a horseshoe prior when the goal is to obtain findings that generalize to new data.
<!-- **In mijn ogen waren de verschillen in performance niet heel groot, dus ik vraag me af of het nu niet iets te sterk is opgeschreven. Maar jij bent meer bezig geweest met de simulatie, dus mogelijk zie ik iets over het hoofd.** -->

With regard to variable selection, results indicated that the penalized BRMA algorithms had lower sensitivity: they were less able to select relevant moderators than RMA.
Conversely, the BRMA algorithms had better specificity: they were better able to reject irrelevant moderators than RMA.
These results are unsurprising because the BRMA algorithms shrink all regression coefficients towards zero.
This diminishes their ability to detect true effects and aids their ability to reject irrelevant moderators.
Importantly however, the overall accuracy was approximately equal for RMA and BRMA with a horseshoe prior.
This means that the total number of Type I and Type II errors will be approximately the same when choosing between these two methods - but there is a tradeoff between sensitivity and specificity.
Applied researchers must consider whether sensitivity or specificity is more important in the context of their research.
When meta-analyzing a heterogeneous body of literature, with many between-study differences that could be coded as moderators, BRMA may be preferred due to its greater ability to retain only relevant moderators.
Conversely, when meta-analyzing a highly curated body of literature with a small number of theoretically relevant moderators, un-penalized RMA might be preferred.

With regard to the algorithms' ability to recover population effect sizes of moderators,
we observed that BRMA with a horseshoe prior had the greatest bias towards zero across simulation conditions, followed by LASSO, and then RMA.
Note that all algorithms provided, on average, negatively biased estimates.
The variance of the estimates followed the opposite pattern.
This illustrates the bias-variance trade-off,
of which the BRMA algorithms' greater predictive performance is a direct consequence.

With regard to residual heterogeneity, we observed that BRMA with a horseshoe prior had the lowest bias.
The BRMA algorithms also had lower variance.
This suggests that the penalized regression coefficients do not compromise the estimation of residual heterogeneity. Future research might investigate under what conditions residual heterogeneity is estimated more accurately in a penalized model than in an unpenalized model.
Together, these results suggest that BRMA has superior predictive performance and specificity, and provides relatively unbiased estimates of residual heterogeneity, relative to RMA.

We examined the effect of several violations of model assumptions, including simulating data from a cubic model.
In applied research, it is often not known what the true shape of the association between a moderator and effect size is.
Thus, model misspecification is likely to occur.
One advantage of BRMA is that it can accommodate more moderators than RMA and has superior specificity.
This allows researchers to specify a more flexible model to account for potential misspecification, with less concern for overfitting and nonconvergence.
For example, researchers could add polynomials of continuous variables with suspected non-linear effects, or interactions between predictors.
If nothing is known about the shape of the associations between moderators and effect size,
non-parametric methods like random forest meta-analysis may be preferable over linear models [@vanlissaSmallSampleMetaanalyses2020].

## Strengths and future directions

The present paper has several strengths.
First, we included a wide range of simulation conditions, including conditions that violated the assumptions of linearity and normality.
Across all conditions, BRMA displayed superior predictive performance and specificity compared to RMA.
Another strength is that the present simulation study used realistic estimates of $\tau^2$,
based on data from 705 published psychological meta-analyses (Van Erp et al., 2017).
Another strength is that the BRMA algorithms have been made available in a FAIR (Findable, Accessible, Interoperable and Reusable) repository: an R-package published on the "Comprehensive R Archive Network".
Thanks to the use of compiled code, the BRMA algorithm is computationally relatively inexpensive. 

Several limitations remain to be addressed in future research, however.
One limitation is that, by necessity, computational resources and journal space limit the number of conditions that could be considered in the simulation study.
To facilitate further exploration and follow-up research,
we have made all simulation data and analysis code for the present study available online.
This code can also be used to conduct Monte Carlo power analyses for applied research.
A second limitation is that the present study did not examine the effect of multicollinear predictors.
Regularizing estimators typically have an advantage over OLS regression in the presence of multicollinearity;
future research ought to examine whether this advantage extends to BRMA.
A third limitation is that the present study did not examine the effect of dependent data (e.g., multiple effect sizes per study).
The BRMA algorithm can accommodate dependent data by means of three-level multilevel analysis.
To our knowledge, there are no reasons to expect that dependent data would result in a different pattern of findings than we found for independent data, but future research is required to ascertain this.
A final limitation of the current implementation is that it relies on 95% credible intervals to select relevant moderators.
However, these marginal credible intervals can behave differently compared to the joint credible intervals [@PiironenEtal2017].
A future direction of research is therefore to implement more advanced selection procedures, such as projective predictive variable selection [@PiironenVehtari2017a].
Another direction for future research is the specification of different priors,
aside from the horseshoe and LASSO priors that were examined in this study.
<!-- To facilitate such research, we provide a generalized BRMA function which is not compiled, and can be fully customized with user-specified priors. -->
<!-- The downside of this flexible function is that it is not compiled, and requires the user to set up a compilation toolchain. -->
<!-- Compiling the function thus requires some technological sophistication and is more computationally costly. -->
<!-- Although the use of Bayesian estimation has several advantages, one major downside is that Bayesian models are not directly comparable with frequentist models. -->
A final disadvantage is that Bayesian estimation is typically more computationally expensive than frequentist estimation.
One future direction of research is thus to develop a frequentist estimator for regularized meta-regression.

## Recommendations for applied research

BRMA aims to address the challenge that arises when meta-analyzing heterogeneous bodies of literature, with few studies relative to the number of moderators.
BRMA can be used to identify relevant moderators when it is not known beforehand which moderators are responsible for between-studies differences in observed effect sizes.
To facilitate adoption of this method in applied research, we have published the function `brma()` in the R package `pema`.
Here, we offer several recommendations for its use.
The first recommendation precedes analysis, and relates to
the design of the meta-analysis.
When the search for moderators is exploratory,
researchers ought to be inclusive, but focus on moderators that are expected to be
relevant, including theoretically relevant moderators, as well as moderators pertaining
to the sample, methods, instruments, study quality, and publication type.
In our experience, many applied researchers code such study characteristics anyway, but omit
them from their analyses for lack of statistical power.
Moderators can be continuous or categorical.
Missing data must be accounted for.
The best way to do so is by retrieving the missing information, by contacting authors or comparing different publications on the same data.
If missing data remains, users can either use a single imputation method
or supply multiple imputed data to the `data` argument (see function documentation).
The effect sizes and their variances must be computed using suitable methods;
note that many such methods are available in the R package `metafor` [@viechtbauerConductingMetaanalysesMetafor2010].
With regard to data analysis, we recommend the use of a horseshoe prior by default,
because it demonstrated the best predictive performance and most attractive trade-off between sensitivity and specificity in our simulations.
When estimating the model, it is important to ascertain that the algorithm has converged before interpreting the results.
Stan, the computational back-end of `brma()`, returns warnings and errors if there are any indications of non-convergence.
Additionally, users can obtain trace plots as described in the illustrative example.

When reporting results, researchers should substantiate their decision to explore heterogeneity on both subjective and objective grounds.
The former can be achieved by simply ascertaining that the body of literature
to be meta-analyzed appears to be heterogeneous;
the same rationale commonly used to support the use of random effects meta-analysis (Higgins et al., 2009).
The latter can be accomplished by conducting a random effects meta-analysis without any moderators,
and reporting the estimated $\tau^2$.
Note that significant heterogeneity does not constitute
sufficient grounds, for deciding to explore ignore heterogeneity, for two reasons:
Firstly, because data-driven decisions render any analysis (partly) exploratory, and increase the risk of results that generalize poorly (i.e., are overfit).
The second reason is that tests for heterogeneity are often underpowered when the number of studies is low, and overpowered when it is high, thus limiting their usefulness (see Higgins & Thompson, 2002).
As when conducting RMA meta-analysis, researchers should report both the estimated effect of moderators and residual heterogeneity.
Regression coefficients can be interpreted as usual, but it is recommended that researchers acknowledge that they are biased towards zero.
If all moderators are centered, the model intercept can be interpreted as the overall effect size at average levels of the moderators.
Note that, as BRMA is a Bayesian method, credible intervals or highest posterior density intervals should be used for inference, instead of p-values.
The null hypothesis is rejected if such intervals exclude zero.
As both types of intervals performed identically in the present study,
we suggest using credible intervals, which are computationally less expensive.

Finally, with regard to publication, we highly recommend making the data and code for the meta-analysis publicly available.
One way to do this is by creating a reproducible research repository,
for example, using the Workflow for Reproducible Code in Science [WORCS, @vanlissaWORCSWorkflowOpen2020].
Transparency allows readers and reviewers to verify that methods were correctly applied,
which should inspire greater confidence in the results.
Others can easily perform sensitivity analyses by changing the analysis code.
Sharing data allows the meta-analysis to be updated in the future,
which increases the reuse value of the data.
Finally, sharing the model object (or code to reproduce it) allows others to obtain predictions for the expected effect size of a new study on the same topic.
This prediction can be used to conduct power analysis for future research.
To this end, researchers can simply enter their planned
design (or several alternative designs) as new lines of data, using the codebook of the
original meta-analysis, and use the published BRMA model to calculate the
predicted effect size for a study with these specifications.

BRMA may not be the best solution for every situation.
Several trade-offs must be considered to decide what method is most appropriate.
Firstly, the fact that BRMA has high predictive performance compared to RMA suggests that it is particularly suitable when a researcher intends to obtain results that will generalize beyond the sample at hand,
and is willing to accept some bias in parameter estimates.
Conversely, RMA might be more suitable when the goal is to describe the sample at hand in an unbiased manner,
with less concern for generalizability to future studies.
Secondly, the fact that BRMA has high specificity compared to RMA suggests that it is more suitable when a researcher seeks to eliminate irrelevant moderators at the cost of increasing the Type II error rate.
Conversely, RMA might be more suitable when the researcher seeks to identify relevant moderators, at the cost of increasing the Type I error rate.
If many moderators have been coded, and many of them are expected to be irrelevant, then BRMA may thus be preferable.
Thirdly, there may be pragmatic reasons for preferring BRMA over RMA.
For example, if a dataset is small, or the number of moderators is high relative to the number of cases,
RMA models may be empirically under-identified.
This can be indicated by convergence problems.
In such cases, Bayesian estimation may converge on a solution where frequentist estimation does not [@kohliFittingLinearLinearPiecewise2015].
Similarly, BRMA may perform better in the presence of multicollinearity among predictors,
which can be examined using the function `vif()` in the R-package `metafor`.
Values exceeding 5 are cause for concern.
Multicollinearity increases the variance of regression coefficients.
BRMA may have an advantage here, because the regularizing priors restrict variance.
If multicollinearity is observed, researchers might thus prefer BRMA over RMA.

# Conclusion

The present research has demonstrated that BRMA is a powerful tool for
exploring heterogeneity in meta-analysis, with a number of advantages over classic RMA.
BRMA had better predictive performance than RMA, which indicates that results from BRMA analysis generalize better to new data.
This predictive performance advantage was especially pronounced when training data were as small as 20 studies.
This is appealing because many meta-analyses have small sample sizes.
BRMA further has greater specificity in rejecting irrelevant moderators from a larger set of potential candidates, while maintaining an overall variable selection accuracy equivalent to RMA.
Although the estimated regression coefficients are biased towards zero by design,
the estimated residual heterogeneity did not show evidence of bias in our simulation.
A final advantage of BRMA over other variable selection methods for meta-analysis is that it is an extension of the linear model.
Most applied researchers are familiar with the linear model, and it can easily accommodate predictor variables of any measurement level, interaction terms, and non-linear effects.
Adoption of this new method may be further facilitated by the availability of the user-friendly R package `pema`.

<!-- indicated that the BRMA algorithms had higher predictive performance than RMA in the presence of relevant moderators. -->
<!-- In the absence of relevant moderators, RMA produced overfit models; in other words, its models generalized poorly to new data. -->
<!-- The predictive performance of the BRMA algorithms also suffered less than that of RMA in the presence of more irrelevant moderators. -->
<!-- The BRMA algorithms were also more efficient, in the sense that they achieved greater predictive performance when the number of studies in the training data was low. -->
<!-- Across all conditions, BRMA with a horseshoe prior achieved the highest average predictive performance, and within each data set, BRMA with a horseshoe prior most often had the best predictive performance (in `r round(whichhi[1]*100)`% of replications). -->
<!-- This provides strong evidence that BRMA with a horseshoe prior is generally preferable when the goal is to obtain findings that generalize to new data. -->

<!-- 6. First note the general trend that during estimation of all models $\tau^{2}$ got more overestimated as $\beta$ increased, except during estimation of the linear model, where the effect of $\beta$ on $\Delta\tau^{2}$ was close to zero, except for MetaForest. However, note the scales for the y-axes. While estimating the two-way interaction, linear and exponential model, $\Delta\tau^2$ stayed well within a confined interval. However, the algorithms severely overestimated $\tau^{2}$ when the model contained cubic terms. Especially MetaForest overestimated $\tau^{2}$ substantially when $\beta = 0.8$ and the estimated model is cubic: $\Delta\tau^{2}_{MF} = 2.92 \pm 2.37$. The other algorithms also had a $\Delta\tau^{2} > 1$ in these conditions, but the results were not as severe. Interestingly, the Pema algorithms even outperformed the RMA algorithm in these conditions.  -->

<!-- The marginal effects of $\beta$ on $\Delta\tau^{2}$ are shown in image 5B. MetaForest was affected most by the increase in $\beta$, but in general performed better than the Pema algorithms when $\beta < 0.8$. The RMA algorithm performed best overall. -->

<!-- The marginal effect of $\alpha$ on $\Delta\tau^{2}$ was rather minimal, although there was a slight decrease in $\Delta\tau^{2}$ as $\alpha$ increased. However, the decrease is more explicit when the interaction of $\alpha$ with the estimated model is added. Image 7 shows this interaction. The algorithms were rather unaffected by $\alpha$ for the linear model, and a small decrease in $\Delta\tau^{2}$ as $\alpha$ increased can be seen in the exponential model. When the two-way interaction model was estimated however, the algorithms benefitted as $\alpha$ increased, while for the cubic model, $\Delta\tau^{2}$ first increased as $\alpha$ increased from 0 to 2, but decreased as $\alpha$ increased from 2 to 10. RMA performed best, followed by MetaForest. The Pema algorithms performed similarly, but worst.  -->

<!-- The effect of the true $\tau^{2}$ on $\Delta\tau^{2}$ was rather unnoticeable for the RMA and MetaForest algorithms. The tendency for the Pema algorithms on the other hand, was to overestimate $\tau^{2}$ more as the true $\tau^{2}$ increased. Image 5C shows the marginal relationship. -->

<!-- The effect of the number of moderators on $\Delta\tau^{2}$  was not that large either. A small increase in $\Delta\tau^{2}$ can be seen in the RMA and MetaForest algorithm as the number of moderators increased which was not found for the Pema algorithms. However, A small note is that MetaForest did substantially increase in $\Delta\tau^{2}$ as more moderators were added and the estimated model is cubic. Image 8 shows the interaction between the number of moderators and the estimated model. -->

<!-- $\kappa$ only had a substantial effect for MetaForest; the $\Delta\tau^{2}$  decreased quite rapidly if $\kappa$ increased, especially when the cubic model was estimated. For the other algorithms, decreasing $\kappa$ had little to no effect on correctly estimating the residual heterogeneity. Image 9 shows the interaction of $\kappa$ with the estimated model. -->

<!-- Finally, the average number of observations in the studies did not have a substantial effect on $\Delta\tau^{2}$. Image 5D shows the marginal relationship. -->


\newpage
# Highlights

* Many applied meta-analyses concern heterogeneous bodies of literature, with many between-studies differences (moderators).
* Simultaneously, meta-analytic samples are often small. There is thus limited statistical power to account for moderators.
* The present study introduces Bayesian Regularized Meta-Analysis (BRMA), an algorithm that applies regularization to identify relevant moderators from a larger number of candidates.
* The algorithm is made available in a user-friendly R-package, `pema`, which is published on CRAN.
* Readers across fields can use this method to account for between-studies heterogeneity in meta-analysis, without concern that models may be underfit or underpowered. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup



