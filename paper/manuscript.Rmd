---
title             : "Select relevant moderators in meta-regression using Bayesian penalization"
shorttitle        : "Title"

author:
  - name: "Caspar J. Van Lissa"
    affiliation: "1,2"
    corresponding: yes
    address: "Padualaan 14, 3584CH Utrecht, The Netherlands"
    email: "c.j.vanlissa@uu.nl"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Writing - Original Draft Preparation
      - Writing - Review & Editing    
  - name: "Andreas M. Brandmaier"
    affiliation: "3,4"
    

  - name          : "Ernst-August Doelle"
    affiliation   : "1,2"
    role:
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Wilhelm-Wundt-University"
  - id            : "2"
    institution   : "Konstanz Business School"

authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.

  Enter author note here.

abstract: |
  One or two sentences providing a **basic introduction** to the field,  comprehensible to a scientist in any discipline.
  
  Two to three sentences of **more detailed background**, comprehensible  to scientists in related disciplines.
  
  One sentence clearly stating the **general problem** being addressed by  this particular study.
  
  One sentence summarizing the main result (with the words "**here we show**" or their equivalent).
  
  Two or three sentences explaining what the **main result** reveals in direct comparison to what was thought to be the case previously, or how the  main result adds to previous knowledge.
  
  One or two sentences to put the results into a more **general context**.
  
  Two or three sentences to provide a **broader perspective**, readily comprehensible to a scientist in any discipline.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : ["r-references.bib"]

floatsintext      : no
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


Skeleton lasso/pema paper
1.) What is Meta-analysis?
2.) What is meta-regression and how does it complement meta-analysis?
	a.) Introduce Moderators
	b.) Study Heterogeneity + Random Sampling Error (and their difference)	
2.5.) Fixed vs. random effects
	a.) Shortcomings of fixed effect models
3.) Shortcomings of current meta regressions w.r.t. estimating coefficients and heterogeneity:
	a.) Small sample size / overfitting
	b.) Non-normal data
4.) Various methods to estimate heterogeneity (and coefficients)
	a.) The use of WLS and REML
5.) Intro to Frequentist linear methods/Bayesian methods and Random forests, along with their 
(dis-)advantages:
	a.) Rma: uses WLS for estimation
	b.) MetaForest (Random Effects): Uses Random Forest Algorithm
	c.) Lasso Pema: uses penalized Lasso
	d.) Horseshoe Pema: uses horseshoe priors
6.) Goal of the current study
7.) Means of attaining goal and evaluation of performance:
	a.) simulation study
	b.) algorithmic performance
	c.) design factors
	d.) Impact of design factors on algorithim performance
	e.) Hypotheses of algorithmic performances

colour coding: I colour coded the text as to know from which file the text is copied
GREEN = derived from ‘Thesis_Metaforest’
BLUE = Thesis_lasso
BLACK = internship_report
RED = Inserted myself

# introduction

Meta-analysis is the practice of aggregating effect sizes from multiple similar studies.
In its simplest form, a summary effect is computed as a weighted average of the observed effect sizes. 
The weight assigned to each effect size is based on certain assumptions,
and determines how influential it is in calculating the summary effect.
For example, the *fixed effect* model assumes that all observed effect sizes reflect one underlying true population effect size.
This assumption may be reasonable for close replication studies [@higgins_re-evaluation_2009, fabrigar_conceptualizing_2016, maxwell_is_2015]. 
The *random effects* model, by contrast, assumes that population effect sizes follow a normal distribution.
Each observed effect size provides information about the mean and standard deviation of this distribution of population effect sizes.
This assumption is appropriate when studies are conceptually similar and differences between them are random [@higgins_re-evaluation_2009, fabrigar_conceptualizing_2016, maxwell_is_2015]. 

Aside from random heterogeneity in effect sizes, quantifiable between-study differences may introduce systematic heterogeneity.
Such between-study differences are known as "moderators".
For example, if studies have been replicated in Europe and the Americas, this difference can be captured by a binary moderator called "continent".
Alternatively, if studies have used different dosages of the same drug, this may be captured by a continuous moderator called "dosage".
Systematic heterogeneity in the observed effect sizes can be accounted for using *meta-regression* (Viechtbauer \& López-López, 2015).
This technique provides estimates of the effect of one or more study characteristics on the overall effect size,
as well as of the overall effect size and residual heterogeneity after controlling for their influence.

Meta-analysis is often used to summarize existing bodies of literature.
In such situations, the number of moderators is often relatively high because similar research questions have been studied in different laboratories,
using different methods, instruments, and samples.
Each of these between-study differences could be coded as a moderator, and some of these moderators may explain systematic heterogeneity.

It is often not known beforehand which moderators are relevant and which are not.
This introduces a methodological challenge,
because the number of studies in meta-analyses is typically small (Riley, Higgins \& Deeks, 2011),
resulting in low power to identify relevant moderators (Thompson \& Higgins, 2002).
There is thus a need for a technique that can perform variable selection in meta-analysis, and which is relatively robust to small sample sizes.


$$
\begin{aligned}
y_i &= \theta_i + \epsilon_i &\text{where } \epsilon_i \sim N(0, \sigma^2_i)\\ % \text{ and } 
%\theta_1 &= \theta_2 \dots = \theta_k
\end{aligned}
$$

., and methods have been developed to account for their influence. However, extant approaches lack the power to assess more than a handful of known moderators, or to investigate interactions between moderators, and non-linear effects. Heterogeneity between studies thus presents a non-trivial challenge to data aggregation using classic meta-analytic methods. At the same time, it also offers an unexploited opportunity to learn which differences between studies have an impact on the effect size found, if adequate exploratory techniques can be developed. 





Thus, there is a need of a regularization method to curtail overfitting. Least Absolute Shrinkage and Selection Operator [LASSO] (L1-norm regularization) can fulfill this role, since it has an advantage in terms of feature selection. The goal of this project is to implement L1-norm regularization in the weighted meta-regression, developing an new estimator for penalized meta-regression.


**Fixed vs Random effects**
The two classic approaches of meta-analysis refer to fundamental different assumptions made about the underlying data. These assumptions define the weights and will also determine which methods are used for the weighting of individual studies and for the creation a summary effect.

The first approach is referred to as the as the fixed-effect model. This model assumes that each observed effect size, obtained from each individual study, is an estimate of an underlying true effect size (Hedges \& Vevea, 1998). The true effect sizes are treated as, unknown, constants. The only source that causes the deviation of the observed effect from the, unknown, true effects is sampling error. Thus, for a collection of $k$ studies, the observed effects size $y_{i}$ of each individual study $i$ (for $i$ = 1,2, . . . $k$) is given by:

 $y_{i}$ = $\theta$ + $\varepsilon_{i}$						(1)

Where $\theta$ is the true effect size of each individual study $i$ and $\varepsilon_{i} \sim \mathcal{N}(0,\,\sigma_{i})\,$ with $\sigma_{i}$ being the sampling error or within-study variance, which is treated as a known factor.

Fixed-effects meta-analysis considers sampling error to be the only cause of variance that influences the observed effect size. Studies with a large sample size will, as a result of this, produce more precise estimations of the underlying true effect size (Schmidt, Oh \& Hayes, 2009). Therefore, large sample sizes will contribute more to the weighted mean than small sample sizes. With that said, fixed-effects weights are defined by van Lissa (2017): “as the reciprocal of the effect size variances”:

 $W_{i} =  \frac{1}{\sigma^2_{i}}$						  (2)
			   
For an accurate estimate of the fixed-effects meta-analysis model, it starts with the assumption that the true effect is even in al studies. However, this assumption is implausible in plenty of meta-analysis. Especially in social sciences, the behaviour of people is extremely diverse and the contextual conditions for all humans vary to a great degree (Aronson, Wilson \& Akert, 2016). A highly likely consequence is that this will lead to a huge amount of possible moderators (Caserio, 2014). Also studies that examine same or similar research questions often differ. Caused by differences in for example cultures of research populations and used methods or instruments (Neuman, 2011). Even in replication studies there are sometimes moderators that are unanticipated (Kunert, 2016). This leads very often to an eventual poor performance of the fixed-effects meta-analysis model (Snijders, 2005). 
	
The second model is the random-effects model. This approach makes an additional assumption, namely about the true effect sizes. Where the fixed-effect model treats the true effects as constants, the random-effect model assumes that the true effects are random and follow a distribution of their own (Hedges \& Vevea, 1998). This means that variation in the observed effects ($y_{i}$) in the random model incorporates not only the sampling error but also the variation of the true effect sizes $\tau^{2}$ between the studies. In the case of the random effect model the observed effect size of $y_{i}$ is, given by:

 $y_{i} = \theta_{i}+\varepsilon_{i}$ 						(3)
        
With $\varepsilon_{i} \sim \mathcal{N}(0,\,\sigma_{i})\,$ but, in this case $\theta_{i}$ on itself is given by:

 $theta_{i} = \mu + \zeta_{i}$						(4)

With $\mu$ being the mean of the distribution of the true effect sizes and $\zeta_{i} \sim \mathcal{N}(0,\,\tau^{2})\,$ with $\tau^2$ being the variance of the population of true effect sizes. It could also be explained as the variance between the individual studies. 
However, in the case of random-effects, the true effects also follow a distribution, so therefore the between study variance is also taken into account when composing the weights for the individual studies. The individual weights for the random-effect model are given by:

 $W_{i} =  \frac{1}{\sigma^2_{i} + \tau^{2}}$						(5)
In the case of the random-effect model, the within-study- and between-study variance is necessary for the calculation of the weights. It is important to note that in the calculation of the individual weights, an estimation of study heterogeneity is used $\tau^{2}$. While the sampling error is known for each individual study, the true effect heterogeneity $\tau^{2}$ remains unknown. Therefore, an estimation of the heterogeneity value needs to be made to effectively calculate the weights. This estimation of the between-study variance is thus represented by $\tau^{2}$.

**Meta-regression**
In the case of fixed- and random-effect meta-analysis, the observed effects are treated as estimations of the underlying true effect. In meta-regression the observed effects are estimated by the including the moderators. In other words, the true effect is now replaced by the moderator effects. This is expressed with the following equation, where $\theta_{i}$ represents the underlying true effect, $x_{i}$ the moderators, $\beta_{i}$ the coefficients, with $p$ being the number of moderators:

$\theta_{i}=\beta_{0}+ \beta_{1}x_{1}+ \beta_{2}x_{2} + \ldots + \beta_{p}x_{p}+ \zeta_{i}$			(6)

When this is substituted in the original equation it will result in:

$y_{i}=\beta_{0}+ \beta_{1}x_{1}+ \beta_{2}x_{2} + \ldots + \beta_{p}x_{p}+ \zeta_{i} + \varepsilon_{i}$			(7)

The error term $\zeta_{i}$ captures the residual heterogeneity after accounting for the moderators. This term is still included because it is often the case that there still remains heterogeneity unexplained after accounting for the moderators (Thompson \& Sharp, 1999). In this model the moderator effects are treated as fixed and the residual heterogeneity as random. Therefore, it is referred to as a mixed-effect meta-regression analysis model, in short, ME-MRA (Viechtbauer \& López-López, 2015). To solve this ME-MRA model, both the residual heterogeneity and the moderator coefficients need to be estimated. An accurate estimation of the residual heterogeneity contributes to a better interpretation of the effect of the moderators (Panityakul, Bumrungsup \& Knapp, 2013).

**Estimating residual heterogeneity**
The topic of estimating the residual heterogeneity is a highly discussed one (Veroniki et al., 2016; Viechtbauer \& López-López, 2015; Panityakul et al., 2013). The ability of the estimators to predict the residual heterogeneity is influenced by different factors, such as the number of studies (Guolo \& Varin, 2017; Panityakul et al., 2013; Hardy \& Thompson, 1996) included and the sample size of the individual studies (Panityakul et al., 2013).
A third, and obvious factor, that is classified as relevant to model performance is heterogeneity among studies being meta-analysed (Kontopantelis \& Reeves, 2011; Jackson \& White, 2018). Coverage from models degrades when the residual heterogeneity increases, mostly when the amount of studies is small (Brockwell \& Gorden, 2001). Considering that all models their performance is linked to the accuracy of the estimate. According to Sidik \& Jonkman (2007), it is generally the case that the larger true between-study variance is, the more biased the estimate can be, which diminishes the performance of the method.

**Methods for estimating residual heterogeneity**
Numerous methods have been proposed to accurately estimate the residual heterogeneity, including the Hedges (HE), DerSimonian–Laird/Method of Moments (DL), Sidik and Jonkman (SJ), Maximum Likelihood (ML), Restricted Maximum Likelihood (REML), and Empirical Bayes (EB) method. These methods are mostly divided into two groups: closed-form or non-iterative methods and iterative methods. The main difference between these groups is that the closed form group uses a predetermined number of steps to provide an estimation for the residual heterogeneity, whereas the iterative methods run multiple iteration, as the name suggests, to converge to a solution when a specific criterion is met. It is important to note that some iterative methods do not produce a solution when they fail to converge after a predetermined amount of iteration. 

In our scenario we are especially interested in an estimator which performs well under the condition of a relative low number of studies. The Restricted Maximum Likelihood (REML) seems to produce the lowest bias under this condition and is therefore preferred (Panityakul et al., 2013; Hardy & Thompson, 1996). The REML is an iterative method and needs a starting estimation of $\tau^{2}$ to start, usually it gets estimated by one of the non-iterative methods (Viechtbauer \& López-López, 2015). Besides the starting value of $\tau^{2}$, it needs in every iteration an estimation of the regression coefficients of the moderators. These are typically estimated by using the Weighted Least Squares (WLS) method. This is a variation of the Ordinary Least Squares (OLS), but in the case of meta-analysis it is necessary to assess weights to the coefficients. In systematic reviews large variation in standard errors is often observed, which will result in large heteroscedasticity in the estimation of the effects (Stanley \& Doucouliagos, 2017). The addition of weights is a way to adjust for this heteroscedasticity. The weights are formulated as presented in equation (5). 

The usage of a WLS method to estimate the regression coefficient may be problematic in the situation where a lot of moderators are measured without their specific effects, when the amount of studies is low and when moderators are dichotomous. The use of a least squares method will cause problems with the prediction accuracy and the model interpretability (James, Witten, Hastie, \& Tibshirani, 2013). In the situation where a lot of moderators are measured and blindly included in the model, it may as well be the case that variables are included that are in fact not associated with the response. Including irrelevant variables in the model lowers the interpretability of the model (James et al., 2013). An approach is necessary that automatically excludes the variables that are irrelevant i.e. performs variable selection. As explained before, in meta-analysis it is often the case that the number of moderators closely approaches or even exceeds the number of studies included in the analysis. A least squares method will display a lot variability in the fit when the number of variables is not much smaller than the number of studies (James et al., 2013). This means that the least squares method over fits the data and loses its power to be generalizable to future observations. When the number of variables exceeds the number of studies, the least squares method fails to produce one unique estimate and the method should not be used at all. 

However, a least squares method could still be somewhat valuable in some situations. It is extremely suitable to estimate a linear relationship. In the case of dichotomous moderators, the relationship is always perfectly linear. A powerful non-linear estimation tool is in the situation of dichotomous moderators unnecessary and would not perform better at all. Whenever a non-linear relation gets fitted on data with an underlying linear relation, it will cause problems when this fit gets used for the prediction of future data. Given the various arguments, this paper provides an approach to tackle this problem of the least squares methods whilst still making use of a linear method. The weighted least squares are replaced with the so-called LASSO regression for the estimation of the regression coefficients. This algorithm shrinks or penalizes the regression coefficients and performs variable selection (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). 


**Intro rma**
The rma algorithm is part of the software-package `metafor` in `R`, which is developed by Wolfgang Viechtbauer (2010, 2019). This algorithm is specifically developed to perform a meta-analysis or meta-regression. It allows to include different models, such as the fixed-, random- and mixed-effect model. It is also possible to account for moderators (Viechtbauer, 2010). The mixed-effect model, which is used is this study, requires a two-step approach to fit a meta-analytic model. First the residual heterogeneity is estimated. The package developed by Viechtbauer does provide multiple methods for the estimation of the residual heterogeneity. In this study the Restricted Maximum-likelihood is used, but this has already been discussed earlier. The second step is estimating the moderator coefficients, which is done by using the Weighted Least Squares (WLS) method. The weights are described in equation (5). The lma is a variation of the rma algorithm which is created by Caspar van Lissa. As explained before, the REML is an iterative procedure for the estimation of the residual heterogeneity. In every step of the process, instead of estimating the coefficients of the moderators by using a WLS, a weighted lasso regression is performed. Then again, the residual heterogeneity gets estimated with the rma algorithm by using the new values of the coefficients. With these new values of $\tau^{2}$, a new weighted lasso is performed for the estimations of the coefficients. This process continuous, until the residual heterogeneity converges to a certain value. 

**Intro Lasso**
The lasso is a technique that regularizes or constrains the coefficient estimates, better known as shrinking (James et al., 2013). It possesses the ability to reduce the regression coefficient even to a value of zero. By doing this it automatically performs variable selection. It does not seem to be immediately clear why shrinking the coefficients should be an improvement to the model. However, by shrinking the parameters, it lowers the variance of the model by increasing the bias only a little bit. In other words, the model sacrifices some of its ability to fit the current data, to greatly increase the ability to predict future data with the same fit (James et al., 2013). This is better known as the bias/variance tradeoff (Briscoe & Feldman, 2011). 

The Lasso shrinkage method is not the only shrinkage method, there do exist some others. Nevertheless, the lasso is in the case the best option. It possesses, as opposed to other methods, the ability to shrink the parameter not towards zero, but to be exactly zero (James et al., 2013; Hesterberg, Choi, Meier, \& Fraley, 2008). This means that the lasso can perform variable selection, something that is specifically aimed for in this study. 

In line with other shrinkage methods the lasso makes use of a shrinkage penalty. This penalty is added in the process of the OLS calculation of the regression coefficients. The OLS method estimates the coefficients by minimizing the Residual Sum of Squares (RSS). The following equation shows how the calculation of the RSS together with the shrinkage penalty:

$RSS=\sum_{i=1}^{n}(y_{i} - \beta_{0} - \sum_{j=1}^{p}\beta_{j}x_{ij})^2 + \lambda\sum_{j=1}^{p}|\beta_{j}|$			(8)

This equation shows that the shrinkage penalty consists of two variables, the tuning parameter $\lambda$ and the regression coefficients $\beta_{j}$. This means that, while the OLS tries to find the coefficients which explain as much variance as possible, due to the minimization of the RSS, the shrinkage penalty punishes this. Therefore, the coefficients are forced to shrink a certain amount, depending on the parameter $\lambda$. If the $\lambda$ increases, it grows the impact of the shrinkage penalty on the RSS, with $\lambda \to \infty$ shrinking all the coefficient to be zero, producing the null model. But, if the $\lambda$ is zero, the shrinkage penalty has no impact at all and it will produce the OLS estimates.

**Alternative to linear model: Tree Based models**
An alternative that can perform variable selection, are tree-based models. These kinds of models have numerous other advantages over linear models. Tree-based models can be used for any data type, are easy to represent visually, require little data preparation and got larger power than linear regressions when moderators exceed observations in quantity. They are also more flexible in handling moderator interactions and non-linearity. As a result of that, they are better in modelling the complicated nature of human behaviour (Earp \& Trafimow, 2015). Decision trees split from the top down and group data in so-called ‘sub-nodes’, in which the data’s aspects are most homogeneous. The goal is to split to get the sub-nodes as uniform as possible, which can be until fully homogenous groups, or if a pre-specified touchstone is reached. 
Still, singletree based models have some limitations. First of all, tree models are unstable, small fluctuations that are utilized to make the model have a possibility to lead to considerable alterations in the constructions of the tree (Dwyer \& Holte, 2007). Second, it has problems with seizing linearity, because it only makes ‘twofold splits’ (Steyerberg, 2019). At last, tree-based models are susceptible to overfitting (Hastie et al, 2009). 

There are also more complex tree-based models, known as random-forests, which surmount most of the disadvantages of singletree. This variant incorporates multiple decision trees, and combines results from those trees to create a single model with a more accurate estimate (Breiman, 2001). The essential idea behind is know as the ‘wisdom of crowds’, a large number of relatively uncorrelated trees operating as a group will outperform any of the individual elements. The somewhat low correlation between the models is fundamental, because uncorrelated models are able to produce ensemble predictions with a higher accuracy that any individual prediction. This is because the trees preserve each other from their own singular errors (Genuer, Poggi \& Tuleau-Malot, 2010). The lower tendency to overfitting is another advantage of random forests over single trees (Bühlmann \& Yu, 2002). As well as the possibility to predict cases that are not components of the bootstrap sample of the tree. This kind of measure is known as out-of-bag error, which is an approximation of the cross-validation error, and provides proper estimates of the prediction accuracy in further samples (Hastie et al., 2009). 
An alternative to explore heterogeneity in meta-analysis with a singletree-based method is MetaForest. A technique developed by van Lissa (2017), designed to overcome the lacking’s of singletrees by using random forests. MetaForest applies random-effects or fixed-effects weights to random forests.
Based on two simulation studies, van Lissa (2017) examined the performance of fixed-effects, random-effects and unweighted MetaForest.

The study displayed also other advantages from random forests over singletrees. It had greater power, was able to make better predictions, gave estimates of the cross-validation error and yielded useful measures of variable importance and partial prediction plots (van Lissa, 2017). MetaForest can at the moment be considered as the best working technique to explore heterogeneity in meta-analysis. In van Lissa (2017), that only presented estimates of $\tau^{2}$ based on the raw data, we saw that MetaForest had certain robustness against a low number of studies. If moderators were continuously distributed, MetaForest had sufficient power at approximately 20 studies. However, there is an important feature to prove before we can make such an assumption. The underlying data generating models in the two simulation studies of van Lissa (2017) only included normal distributed moderators. Renouncing from normal distributions may affect the performance of the model, but since normal distribution in real-life data is more an exception than a normal state of affairs (Micceri, 1989), it is entirely possible that procedures are affected by skewness, leverage, balance etc. It is important to know how MetaForest performs in these kinds of situations.

**Algorithms and simulation + goal study**
The goal of the present study was to test whether a ME-MRA model with the lasso algorithm is able to outperform the ME-MRA with least squares regression. More specifically, if the lasso is able to outperform the least squares when in situation where the amount studies included in the analysis is fairly low. To test this, two different algorithms are used; one called the rma, which makes use of the WLS regression, and the lma, which makes use of a penalized lasso regression.
To test the lma and rma algorithms on the performance criteria, a simulation study is performed. A simulation of the data is preferred over the use of real data. Simulated data can be shaped to such an extent that it will have the all desired characteristics to test the performance of the algorithm. Besides that, if simulated correctly, it will not have any systematic errors or noise due to underlying models and it is more cost efficient. Simulations are useful for evaluation of new methods like MetaForest and for the comparison with alternative methods like metaCART and the classic approaches. 

**Performance Criteria**
The algorithms are evaluated on three different performance criteria: The algorithms’ predictive performance, their ability to estimate the residual heterogeneity and their ability to detect and select the right moderators.
The predictive performance of the algorithms is defined by how well the algorithm is able to predict future data. The algorithms have to estimate a model on a “training” dataset and then use this model to see how well it fits on a second “testing” dataset. This is operationalized as the cross-validated $R_{cv}^{2}$ (Van Lissa, 2017). The $R_{cv}^{2}$ is calculated using the fraction of variance explained by the model on the testing dataset, relative to faction of variance explained by the mean of the testing dataset. The mean of the testing dataset is the best prediction for the testing data when there is no model present (van Lissa, 2017). The calculation of$R_{cv}^{2}$is expressed by the following equation:

$R_{cv}^{2} = 1- \frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}{\sum_{i=1}^{n}(y_{i}-\bar{y}_{i})^{2}}$					(9)

With $n$ being the number of studies in the testing dataset, $\hat{y}_{i}$ being the estimation for study $i$, and $\bar{y}_{i}$ being the mean of the training dataset. 

The ability of the algorithms to estimate the residual heterogeneity is by simply taking the value of $\tau^{2}$ which the algorithm produces. The true value of the residual heterogeneity is subtracted of the estimated value, solely to make the values more interpretable. This means that a correct estimation of the residual heterogeneity will be expressed by a value which exactly or close to zero. The residual heterogeneity is used as a performance criterion because it is suspected that the lma model might not always be able to predict residual heterogeneity correctly. 

Variable selection is defined in terms of the algorithms ability to accredit positive variable importance values to relevant moderators. Variable importance measures capture the relative contribution of various moderators.	

**Design factors**
In the simulation study, meta analytic datasets will be simulated. These datasets consist of two separate sub-datasets, a training- and a testing dataset. Both sub-datasets will have the same characteristics with the exception of the number of studies included. Certain characteristics of the sub-datasets will be manipulated to test how well the algorithms perform under certain conditions. For each combination of characteristics, or design factors, 100 datasets will be simulated. The design factors that will be manipulated are the number of studies in the training data $k$ (22, 40 and 80), the average within-study sample size $\bar{n}$ (40, 100 and 200), the population effect size $\beta$ (.2, .5 and .8) and the residual heterogeneity $\tau^{2}$ (.01, .04 and .1). All the datasets will contain 20 moderators of which 10 are relevant and 10 are irrelevant. The moderators are binary and follow $\sim \mathcal{Bern}(0.5)$, which corresponds to an equal chance of being either one or zero. The dependent variable $y_{i}$ represented by a $Hedges’ g$. This is an estimator which takes the standardized mean difference between a treatment and control group and is commonly used in meta-analysis (Van Lissa, 2017). The true effect size $\theta_{i}$  is sampled out of a normal distribution. The mean is computed by the assessing the values of the coefficients $\beta_{j}$, with the values of the moderators and with the residual heterogeneity $\tau^{2}$ (Van Lissa, 2017).  This is in line with the calculation of $\theta_{i}$ represented in equation (6). The sampling error $\sigma^{2}_{i}$ is formed by varying the sizes of the samples of each study. The sample sizes $n_{i} \sim \mathcal{N}(\bar{n}, \frac{\bar{n}}{3}$ (Van Lissa, 2017). 

Data were simulated using the random-effects model, based on four models:
(A) Main effect of one moderator, $\mu_{i}= \beta_{1}x_{1i}$ 
(B) Two-way interaction, $\mu_{i}= \beta_{1}x_{1i} + \beta_{2}x_{2i} + \beta_{3}x_{1i}x_{2i}$ 
(E) Non-linear, cubic relationship, $\mu_{i}= \beta_{1}x_{1i} + \beta_{2}x_{1i}^{2} + \beta_{3}x_{1i}^{3}$ 
(F) Exponential relationship, $\mu_{i}= \beta_{1}e^{x_{1i}}$ 

**Impact of design factors and hypotheses**
These design factors are chosen on purpose, because they are hypothesized to have an influence on the predictive performance of the algorithms. The effect of the design factors ought to be either positive or negative on the data. This means that some factor should, by increasing, make the data easier to be analyzed, or make it more difficult to analyze. The amount of studies included in the training data $k$ has a positive influence on the variance explained by the different algorithms. This is due to the fact that there are simply more data points available to fit a model on. The lma algorithm should be superior on the low value of $k$ over the rma algorithm. The effect size $\beta$ has a positive impact on the ability of the algorithms to explain variance. It can be hypothesized that the lma performs better at lower values of $\beta$ because it is better equipped to detect and select variables when even when the amount of signal is low. The residual heterogeneity $\tau^{2}$ should have a negative influence on the interpretability of the data. Differences between the two algorithms could be present, but it remains unclear which would perform better. The lma might perform better when the amount of signal in the data is low or the noise is high, but it is also suspected to overestimate the amount of heterogeneity and this could worsen if the $\tau^{2}$ increases. The $\bar{n}$ greatly influences the quality of the data. Higher values of within-study sample sizes reduce the sampling error. This will lead to a better prediction by the algorithms. In conclusion: higher values of $k$, $\beta$ and  $\bar{n}$  will increase the quality of the data, where higher values of  $\tau^{2}$ decrease the quality of the data. The lma is suspected to perform significantly better when the quality of the data is low, especially when the amount of studies in the sample is low, with the exception of the performance of the lma on the estimation of the residual heterogeneity


# Results
There were 3888 condition combinations and for each of those the algorithms ran 100 times, leaving 388800 cases to analyze. Out of those, 20 cases had missing values on all metrics for the RMA algorithm. Closer inspection showed that both the cubic and exponential model each contributed ten times to the missing values and only when 2, 3 or 6 moderators were taken up in the model. However, since these 20 cases make up only 0.005% of the data, the missing values were chosen to be omitted from further analysis. Another observation is that the two-way interaction model only had results when there were either 3,4 or 7 moderators taken up, while the other models only had results when there were 2,3 or 6 moderators. It is therefor more challenging to compare and interpret the effect the moderators had on the performance criteria between the two-way interaction and the other models.

## Predictive performance
Predictive performance was operationalized by calculating the $R^2_{test}$ and $MSE_{test}$ for every combination of design factors, in further test denoted as $R^2$ and $MSE$ respectively. The densities for the $R^{2}_{test}$ and $MSE_{test}$ values were skewed however, which is why it was chosen to use the median $R^2$ as the metric for predictive performance, rather than the mean. The spread of the metrics was described using the Mean Absolute Deviation [MAD], rather than the standard deviation. It was found that the Horseshoe, Lasso and RMA algorithm performed similarly overall, $R^2_{Hs} = 0.51 \pm 0.36$, $MSE_{Hs} = 0.21 \pm 0.18$ ; $R^2_{Lasso} = 0.50 \pm 0.37$, $MSE_{Lasso} = 0.21\pm 0.19$; $R^2_{RMA} = 0.50 \pm 0.37$, $MSE_{RMA} = 0.22 \pm 0.23$ . The MetaForest algorithm performed worst on $R^2$: $R^2_{Mf} = 0.35 \pm 0.38$, $MSE_{Mf} = 0.22 \pm 0.19$ 

To determine the effect of the design factors on $R^2$ for all algorithms, four separate ANOVA’s were performed, one per algorithm. The effect size $\eta^{2}$ per condition per algorithm, including for all two-way interactions can be found in table 1. Do note that the ANOVA's were performed with the normality assumption violated. The estimates serve mostly as a guidance, rather than an absolute result. 

Not too surprisingly, It was found that the true effect size $\beta$ had the largest effect on $R^{2}$ for all algorithms. As $\beta$ increased, the performance of all algorithms increased as well. However, $\beta$ did interact with the model that was estimated, being either a linear, two-way interaction, cubic or an exponential model. The image of the interaction is shown in image 1. When the exponential and cubic model were estimated, the increase of $R^{2}$ slows for every higher value of $\beta$. For the estimation of the two-way interaction model the increase only slowed when $\beta$ goes from 0.5 to 0.8, while the steepest increase for the linear model estimation is when $\beta$ increases from 0.2 to 0.5. Also noteworthy is that $R^2$ stagnates during estimation of the cubic model as $\beta$ goes up to 0.5 and onwards, while the other models do keep increasing. There was little difference in median $R^2$ between Horsehoe, Lasso and RMA, while MetaForest performed worst.

The second largest marginal effect was that of the estimated model. All algorithms had the highest $R^2$ under the cubic model, followed by a similar performance on the two-way interaction and exponential models. All algorithms performed worst for the linear model. There again was little difference in performance between the Pema algorithms and RMA, altough MetaForest performed worst. Image 2 shows the relationship. 

There also was a moderate interaction effect between the estimated model and the amount of skewness of the input data $\alpha$, especially for the Pema algorithms. Again, Pema and RMA algorithms performed best, followed by MetaForest in all conditions. Most obvious to note is that all algorithms, except for when the linear model was estimated, generally performed better on more skewed data, although the algorithms did perform worse during estimation of the two-way interaction model and as $\alpha$ goes from 5 to 10. Overall performance was best during estimation of the cubic model, but the performance difference between estimated models decreased as $\alpha$ increased. Image 3 shows the relationship.

The true residual heterogeneity $\tau^{2}$ had a negative linear relationship for all algorithms on $R^{2}$. That is, as $\tau^{2}$ increased, $R^{2}$ decreased. Image 4 shows the relationship.

The mean sample size per study $\bar{n}$ also had a moderate effect. For all algorithms the effect of $\bar{n}$ was positively linearly related with $R^{2}$. Again, the Pema and RMA algorithms performed better than MetaForest. Image 5 shows the relationship.

An especially large effect was found for the number of studies used in the training data $\kappa$ for MetaForest, $\eta^{2} = 0.27$, while this effect was substantially smaller for RMA, $\eta^{2} = 0.11$; Lasso, $\eta^{2} = 0.06$ and; Horseshoe, $\eta^{2} = 0.05$. The relationship is positively linear for all algorithms, but the slope is especially steep for MetaForest. Image 6 shows the relationship.

Finally, the number of moderators did not have a big effect for the Pema algorithms: $\eta^{2} = 0.01$ while for RMA and MetaForest $\eta^{2} = 0.05$. The relationship is shown in image 7. The relationship is generally negative with more moderators meaning worse performance, although an increase can be observed as the number of moderators increase from 4 to 6 for all algorithms except MetaForest. This increase in performance for MetaForest appears when the number of moderators go from 3 to 4.

## Estimating residual heterogeneity
The ability of the algorithms to correctly estimate $\tau^{2}$ was operationalized by subtracting the true value for $\tau^{2}$ from the $\tau^{2}$ estimated by the algorithms. Again, the median and Mean Absolute Deviation were used as metrics for performance. The RMA algorithm showed the best results, $\Delta\tau^2_{RMA} = 0.02 \pm 0.06$, followed by the MetaForest algorithm $\Delta\tau^2_{Mf} = 0.09 \pm 0.13$. The Pema algorithms performed worst $\Delta\tau^2_{Lasso} = 0.23 \pm 0.18$; $\Delta\tau^2_{Hs} = 0.23 \pm 0.17$. The finding that all medians are positive implies that all algorithms have a bigger tendency to overestimate $\tau^{2}$ than underestimate it. One comment to make is that uncertainty of the estimates generally increased as $\Delta\tau^{2}$ also increased. This implies that there was more variation in performance as median performance worsened.

To determine the effect of the design factors on $\Delta\tau^{2}$ for all algorithms, four separate ANOVA’s were performed. The effect size $\eta^{2}$ per condition per algorithm, including $\eta^2$ for all two-way interactions can be found in table 2. Again, the assumption of normality was violated.

The biggest predictor on the correct estimation of $\tau^{2}$ was the estimated model. This was mainly because the algorithms overestimated $\tau^{2}$ most when the model contained cubic terms. Image 8 shows the marginal relationship of the estimated model on $\Delta\tau^{2}$ . It becomes more clear why this overestimation occurred when showing the interaction between $\beta$ and the model estimated on $\Delta\tau^{2}$, shown in image 9. First note the general trend that during estimation of all models $\tau^{2}$ got more overestimated as $\beta$ increased, except during estimation of the linear model, where the effect of $\beta$ on $\Delta\tau^{2}$ was close to zero, except for MetaForest. However, note the scales for the y-axes. While estimating the two-way interaction, linear and exponential model, $\Delta\tau^2$ stayed well within a confined interval. However, the algorithms severely overestimated $\tau^{2}$ when the model contained cubic terms. Especially MetaForest overestimated $\tau^{2}$ substantially when $\beta = 0.8$ and the estimated model is cubic: $\Delta\tau^{2}_{MF} = 2.92 \pm 2.37$. The other algorithms also had a $\Delta\tau^{2} > 1$ in these conditions, but the results were not as severe. Interestingly, the Pema algorithms even outperformed the RMA algorithm in these conditions. 

The marginal effects of $\beta$ on $\Delta\tau^{2}$  are shown in image 10. MetaForest was most affected by the increase in $\beta$, but in general performed better than the Pema algorithms when $\beta < 0.8$. The RMA algorithm performed best overall.

The marginal effect of $\alpha$ on $\Delta\tau^{2}$ was rather minimal, altough there was a slight decrease in $\Delta\tau^{2}$ as $\alpha$ increased. However, the decrease is more explicit when the interaction of $\alpha$ with the estimated model is added. Image 11 shows this interaction. The algorithms were rather unaffected by $\alpha$ for the linear model, and a small decrease in $\Delta\tau^{2}$ as $\alpha$ increased can be seen in the exponential model. When the two-way interaction model is estimated however, the algorithms benefitted as $\alpha$ increased, while for the cubic model, $\Delta\tau^{2}$ first increased as $\alpha$ increased from 0 to 2, but decreased as $\alpha$ increased from 2 to 10. RMA performed best, followed by MetaForest. The Pema algorithms performed similarly, but worst. 

The effect of the true $\tau^{2}$ on $\Delta\tau^{2}$ was rather unnoticeable for the RMA and MetaForest algorithms. The tendency for the Pema algorithms on the other hand, was to overestimate $\tau^{2}$ more as the true $\tau^{2}$ increased. Image 12 shows the marginal relationship.

The effect of the number of moderators on $\Delta\tau^{2}$  was not that large either. A small increase in $\Delta\tau^{2}$ can be seen in the RMA and MetaForest algorithm as the number of moderators increased, which was not found for the Pema algorithms. However, A small note is that MetaForest did substantially increase in $\Delta\tau^{2}$ as more moderators were added and the estimated model is cubic. Image 13 shows the interaction between the number of moderators and the estimated model.

$\kappa$ only had a substantial effect for MetaForest; the $\Delta\tau^{2}$  decreased quite rapidly if $\kappa$ increased, especially when the cubic model was estimated. For the other algorithms, decreasing $\kappa$ had little to no effect on correctly estimating the residual heterogeneity. Image 14 shows the relationship.

Finally, the average number of observations in the studies did not have a substantial effect on $\Delta\tau^{2}$. Image 15 shows the marginal relationship.

## Variable selection
To determine the extent to which the algorithms could perform variable selection correctly, the proportion true positives [$TP$] and true negatives [$TN$] were calculated. The $TP$ and $TN$ reflect how well the algorithms accredit importance to relevant moderators and discredit importance to irrelevant moderators respectively. It should be noted that $TP$ could only take on values 1 or 0 per simulated iteration, because in all models where $\beta > 0$, only one moderator was simulated to be relevant. As $\beta$ increased, the already relevant moderator increased in relevance, rather than spreading the relevance over the other moderators. $TN$ had a bigger range and could take on values dependent on how many moderators were taken up in the model. E.g. when $\beta = 0$ and $n_{mods} = n$, $n+1$ different proportions were possible, $\frac{0}{n}$ up until $\frac{n}{n}$.

There were no differences in variables selected by Highest Density Intervals or Confidence Intervals for both Lasso and Horshoe and so for both algorithms it did not matter which interval type was analyzed. It was found that MetaForest had the highest proportion true positives: $TP_{Mf} = 0.98$, closely followed by RMA: $TP_{RMA} = 0.96$. Horshoe performed slightly better than Lasso;  $TP_{Hs} = 0.91$; $TP_{Lasso} = 0.89$. As for $TN$, it was found that the pema algorithms performed best: $TN_{Hs} \ \text{and} \ TN_{Lasso} = 0.93$, followed by RMA: $TN_{RMA} = 0.89 \pm 0.20$. MetaForest performed worst by a large margin: $TN_{Mf} = 0.50$. The Mean Absolute Deviation for all algorithms on both $TP$ and $TN$ was 0, except for MetaForests performance on $TN$, where the Mean Absolute Deviation was 0.44. 

Perfomance on $TP$ and $TN$ were very high for all algorithms, with all mean proportions, except MetaForests performance on $TN$, exceeding .89. This implies that MetaForest had issues excluding irrelevant moderators from the models. Plots were inspected to determine the effects of the design factors on the proportions. While inspecting the plots, there were found to be little marginal effects of the design factors on $TN$, while $TP$ was more affected. 

Firstly, $\kappa$ only had a marginal positive effect on $TP$. As $\kappa$ increased, $TP$ also increased. MetaForest had the highest $TP$, followed by RMA and, lastly, Horsehoe and Lasso. The increase in $TP$ for higher values of $\kappa$ was steeper for the Pema algorithms, however. There was also an interaction of $\kappa$ with the estimated model shown in image 16. During estimation of the linear and two-way interaction model, the relationship of $\kappa$ looked relatively linear. At $\kappa = 20$ the $TP$ was relatively low for the algorithms, compared to the cubic model where $TP$ starts at .98 and converges to 1 as $\kappa$ increased. This latter relationship was also found for the exponential model, although the $TP$ at $\kappa = 20$ was lower.

$\bar{n}$ had a positive and roughly linear relationship with $TP$ for all algorithms. MetaForest performed best, followed by RMA, while Horseshoe and Lasso performed worst. Image 17 shows the relationship.

$\beta$ had an interaction effect with the model estimated on $TN$. Only during estimation of the two-way interaction model, $TN$ decreased as $\beta$ increased, For the other models, $TN$ remained stable. This could be because the interaction model was only fitted when there were 3,4 or 7 moderators, while for the other models, only 2,3 or 6 moderators were used. Image 18 shows the relationship. The effect of $\beta$ on $TP$ was positive; as $\beta$ increased, $TP$ increased too.

The true $\tau^2$ had a negative effect on $TP$, while $\alpha$ seemed to have little effect on both $TP$ and $TN$. Image 19 shows the marginal relationship of $\tau^2$ on $TP$.

Finally, there seemed to be an effect of number of moderators on $TN$, but only for the two-way interaction model. The $TN$ increased as the number of moderators did. Image 20 shows the interaction. This relationship was reversed for $TP$ and was found for all algorithms, i.e. $TP$ decreased as the number of moderators increased. 

# Discussion





\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
